{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write a python program to scrape data for “Data Analyst” Job position in “Bangalore” location. \n",
    "\n",
    "You have to scrape the job-title, job-location, company_name, experience_required. You have to scrape first 10 jobs data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job Title</th>\n",
       "      <th>Company Name</th>\n",
       "      <th>Location</th>\n",
       "      <th>Experience</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Immediate opening For Data Scientist/Data Analyst</td>\n",
       "      <td>CAIA-Center For Artificial Intelligence &amp; Adva...</td>\n",
       "      <td>Chennai, Pune, Bengaluru, Hyderabad</td>\n",
       "      <td>0-3 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Hk solutions</td>\n",
       "      <td>Chennai, Delhi NCR, Bengaluru</td>\n",
       "      <td>0-3 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Intern - DFM Data Analyst</td>\n",
       "      <td>GLOBALFOUNDRIES Engineering Private Limited</td>\n",
       "      <td>Bengaluru</td>\n",
       "      <td>0-5 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hiring Data Analysts on Contract Third party p...</td>\n",
       "      <td>Flipkart Internet Private Limited</td>\n",
       "      <td>Bengaluru</td>\n",
       "      <td>2-6 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Reliability Data Analyst</td>\n",
       "      <td>Alstom Transport India Ltd.</td>\n",
       "      <td>Bengaluru</td>\n",
       "      <td>3-8 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Senior Data Analyst</td>\n",
       "      <td>Myntra</td>\n",
       "      <td>Bengaluru</td>\n",
       "      <td>2-7 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Ladder of changes</td>\n",
       "      <td>Bengaluru</td>\n",
       "      <td>0-5 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Data Analyst / Business Analyst</td>\n",
       "      <td>Altisource</td>\n",
       "      <td>Bengaluru</td>\n",
       "      <td>1-6 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Hiring For Data Analyst RE) - Bangalore</td>\n",
       "      <td>TELEPERFORMANCE GLOBAL SERVICES</td>\n",
       "      <td>Bengaluru(Bellandur)</td>\n",
       "      <td>2-6 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Specialist I / II - Data Analyst</td>\n",
       "      <td>Philips India Limited</td>\n",
       "      <td>Bengaluru</td>\n",
       "      <td>5-7 Yrs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Job Title  \\\n",
       "0  Immediate opening For Data Scientist/Data Analyst   \n",
       "1                                       Data Analyst   \n",
       "2                          Intern - DFM Data Analyst   \n",
       "3  Hiring Data Analysts on Contract Third party p...   \n",
       "4                           Reliability Data Analyst   \n",
       "5                                Senior Data Analyst   \n",
       "6                                       Data Analyst   \n",
       "7                    Data Analyst / Business Analyst   \n",
       "8            Hiring For Data Analyst RE) - Bangalore   \n",
       "9                   Specialist I / II - Data Analyst   \n",
       "\n",
       "                                        Company Name  \\\n",
       "0  CAIA-Center For Artificial Intelligence & Adva...   \n",
       "1                                       Hk solutions   \n",
       "2        GLOBALFOUNDRIES Engineering Private Limited   \n",
       "3                  Flipkart Internet Private Limited   \n",
       "4                        Alstom Transport India Ltd.   \n",
       "5                                             Myntra   \n",
       "6                                  Ladder of changes   \n",
       "7                                         Altisource   \n",
       "8                    TELEPERFORMANCE GLOBAL SERVICES   \n",
       "9                              Philips India Limited   \n",
       "\n",
       "                              Location Experience  \n",
       "0  Chennai, Pune, Bengaluru, Hyderabad    0-3 Yrs  \n",
       "1        Chennai, Delhi NCR, Bengaluru    0-3 Yrs  \n",
       "2                            Bengaluru    0-5 Yrs  \n",
       "3                            Bengaluru    2-6 Yrs  \n",
       "4                            Bengaluru    3-8 Yrs  \n",
       "5                            Bengaluru    2-7 Yrs  \n",
       "6                            Bengaluru    0-5 Yrs  \n",
       "7                            Bengaluru    1-6 Yrs  \n",
       "8                 Bengaluru(Bellandur)    2-6 Yrs  \n",
       "9                            Bengaluru    5-7 Yrs  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing Libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from time import sleep\n",
    "import time \n",
    "import string\n",
    "from random import randint\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "# Loading Driver\n",
    "driver = webdriver.Chrome(r\"C:\\Users\\thunderbolt\\sof\\chromedriver.exe\")\n",
    "\n",
    "#Giving name of the website we have to  scrap\n",
    "url = \"https://www.naukri.com/\"\n",
    "\n",
    "# Getting Url\n",
    "driver.get(url)\n",
    "\n",
    "Job_Title = []\n",
    "Location = []\n",
    "Company_Name = []\n",
    "Experience_Required = []\n",
    "\n",
    "\n",
    "time.sleep(4)\n",
    "\n",
    "driver.switch_to_window(driver.window_handles[1])\n",
    "driver.close()\n",
    "time.sleep(2)\n",
    "driver.switch_to_window(driver.window_handles[2])\n",
    "driver.close()\n",
    "time.sleep(4)\n",
    "driver.switch_to_window(driver.window_handles[1])\n",
    "driver.close()\n",
    "driver.switch_to_window(driver.window_handles[0])\n",
    "\n",
    "time.sleep(4)\n",
    "\n",
    "Task1=driver.find_element_by_xpath(\"//div[@class='privacyPolicy']/div//button\")\n",
    "Task1.click()\n",
    "\n",
    "time.sleep(15)\n",
    "\n",
    "Task2 =driver.find_element_by_xpath(\"//span[@class='fr geoLocBtn later']\")\n",
    "Task2.click()\n",
    "\n",
    "time.sleep(5)\n",
    "\n",
    "Task3=driver.find_element_by_xpath(\"//div[@class='inpWrap']/input\")\n",
    "Task3.send_keys('Data Analyst')\n",
    "\n",
    "time.sleep(5)\n",
    "\n",
    "Task4= driver.find_element_by_xpath(\"//div[2][@class='suggestor-location']//div[@class='sWrap']//input\")\n",
    "Task4.send_keys('Bangalore')\n",
    "\n",
    "time.sleep(2)\n",
    "\n",
    "Task5= driver.find_element_by_xpath(\"//div[@class='qsb-main-content row']//button\")\n",
    "Task5.click()\n",
    "\n",
    "time.sleep(4)\n",
    "\n",
    "Task6 = driver.find_elements_by_xpath(\"//div[@class='jobTupleHeader']/div/a\")\n",
    "\n",
    "# Getting Job title Data.\n",
    "for qw in Task6:\n",
    "    we  = qw.text\n",
    "    Job_Title.append(we)\n",
    "\n",
    "# Putting this data in dataframe.\n",
    "Job_Title = pd.DataFrame(Job_Title)\n",
    "Job_Title = Job_Title.rename({ 0 : 'Job Title'} , axis = 1 )\n",
    "\n",
    "time.sleep(3)\n",
    "\n",
    "\n",
    "Task7 = driver.find_elements_by_xpath(\"//div[@class='info fleft']//a[@class='subTitle ellipsis fleft']\")\n",
    "\n",
    "time.sleep(3)\n",
    "\n",
    "# Getting Company Name \n",
    "for rt in Task7:\n",
    "    yu = rt.text\n",
    "    Company_Name.append(yu)\n",
    "\n",
    "# Putting Data in DataFrame\n",
    "Company_Name = pd.DataFrame(Company_Name)\n",
    "Company_Name= Company_Name.rename({0 : 'Company Name'}, axis=1)\n",
    "\n",
    "Task8 = driver.find_elements_by_xpath(\"//ul[@class='mt-7']//li[@class='fleft grey-text br2 placeHolderLi location']/span\")\n",
    "\n",
    "\n",
    "# Getting location Data\n",
    "for yu in Task8:\n",
    "    ui = yu.text\n",
    "    Location.append(ui)\n",
    "\n",
    "\n",
    "Location =pd.DataFrame(Location)\n",
    "\n",
    "Location=Location.rename( columns = { 0 : 'Location'} )\n",
    "\n",
    "\n",
    "Task9 = driver.find_elements_by_xpath(\"//ul[@class='mt-7']/li[1]//span[@class='ellipsis fleft fs12 lh16']\")\n",
    "\n",
    "\n",
    "# Getting experience required.\n",
    "for ui in Task9:\n",
    "    op = ui.text\n",
    "    Experience_Required.append(op)\n",
    "Experience_Required[0:5]\n",
    "\n",
    "Experience_Required = pd.DataFrame(Experience_Required)\n",
    "Experience_Required  = Experience_Required.rename(columns = { 0 : 'Experience'})\n",
    "\n",
    "\n",
    "Naukri_Jobs = pd.concat([Job_Title,Company_Name,Location ,Experience_Required ] ,axis=1)\n",
    "Naukri_Jobs1 =Naukri_Jobs.iloc[0:10]\n",
    "\n",
    "# Getting our Final Data Analyst Job Data\n",
    "Naukri_Jobs1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write a python program to scrape data for “Data Scientist” Job position in Bangalore” location. \n",
    "\n",
    "You have to scrape the job-title, job-location,company_name, full job-description. You have to scrape first 10 jobs data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job Title</th>\n",
       "      <th>Company Name</th>\n",
       "      <th>Location</th>\n",
       "      <th>Job Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Immediate opening For Data Scientist/Data Analyst</td>\n",
       "      <td>CAIA-Center For Artificial Intelligence &amp; Adva...</td>\n",
       "      <td>Chennai, Pune, Bengaluru, Hyderabad</td>\n",
       "      <td>Job description\\nDear Candidate\\n\\nSchedule a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Hk solutions</td>\n",
       "      <td>Chennai, Delhi NCR, Bengaluru</td>\n",
       "      <td>Job description\\nRoles and Responsibilities : ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Intern - DFM Data Analyst</td>\n",
       "      <td>GLOBALFOUNDRIES Engineering Private Limited</td>\n",
       "      <td>Bengaluru</td>\n",
       "      <td>Roles and Responsibilities\\nSummary of Role:\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hiring Data Analysts on Contract Third party p...</td>\n",
       "      <td>Flipkart Internet Private Limited</td>\n",
       "      <td>Bengaluru</td>\n",
       "      <td>Job description\\nRoles and Responsibilities\\nJ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Reliability Data Analyst</td>\n",
       "      <td>Alstom Transport India Ltd.</td>\n",
       "      <td>Bengaluru</td>\n",
       "      <td>Job description\\nRoles and Responsibilities\\nS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Senior Data Analyst</td>\n",
       "      <td>Myntra</td>\n",
       "      <td>Bengaluru</td>\n",
       "      <td>Job description\\nRoles and Responsibilities\\nR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Ladder of changes</td>\n",
       "      <td>Bengaluru</td>\n",
       "      <td>Job description\\nGreetings from “Altisource bu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Data Analyst / Business Analyst</td>\n",
       "      <td>Altisource</td>\n",
       "      <td>Bengaluru</td>\n",
       "      <td>Job description\\nHiring for Data Analyst(RE)\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Hiring For Data Analyst RE) - Bangalore</td>\n",
       "      <td>TELEPERFORMANCE GLOBAL SERVICES</td>\n",
       "      <td>Bengaluru(Bellandur)</td>\n",
       "      <td>Job description\\nHiring for Data Analyst(RE)\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Specialist I / II - Data Analyst</td>\n",
       "      <td>Philips India Limited</td>\n",
       "      <td>Bengaluru</td>\n",
       "      <td>Roles and Responsibilities\\nTeam Lead - CDM/PV...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Job Title  \\\n",
       "0  Immediate opening For Data Scientist/Data Analyst   \n",
       "1                                       Data Analyst   \n",
       "2                          Intern - DFM Data Analyst   \n",
       "3  Hiring Data Analysts on Contract Third party p...   \n",
       "4                           Reliability Data Analyst   \n",
       "5                                Senior Data Analyst   \n",
       "6                                       Data Analyst   \n",
       "7                    Data Analyst / Business Analyst   \n",
       "8            Hiring For Data Analyst RE) - Bangalore   \n",
       "9                   Specialist I / II - Data Analyst   \n",
       "\n",
       "                                        Company Name  \\\n",
       "0  CAIA-Center For Artificial Intelligence & Adva...   \n",
       "1                                       Hk solutions   \n",
       "2        GLOBALFOUNDRIES Engineering Private Limited   \n",
       "3                  Flipkart Internet Private Limited   \n",
       "4                        Alstom Transport India Ltd.   \n",
       "5                                             Myntra   \n",
       "6                                  Ladder of changes   \n",
       "7                                         Altisource   \n",
       "8                    TELEPERFORMANCE GLOBAL SERVICES   \n",
       "9                              Philips India Limited   \n",
       "\n",
       "                              Location  \\\n",
       "0  Chennai, Pune, Bengaluru, Hyderabad   \n",
       "1        Chennai, Delhi NCR, Bengaluru   \n",
       "2                            Bengaluru   \n",
       "3                            Bengaluru   \n",
       "4                            Bengaluru   \n",
       "5                            Bengaluru   \n",
       "6                            Bengaluru   \n",
       "7                            Bengaluru   \n",
       "8                 Bengaluru(Bellandur)   \n",
       "9                            Bengaluru   \n",
       "\n",
       "                                     Job Description  \n",
       "0  Job description\\nDear Candidate\\n\\nSchedule a ...  \n",
       "1  Job description\\nRoles and Responsibilities : ...  \n",
       "2  Roles and Responsibilities\\nSummary of Role:\\n...  \n",
       "3  Job description\\nRoles and Responsibilities\\nJ...  \n",
       "4  Job description\\nRoles and Responsibilities\\nS...  \n",
       "5  Job description\\nRoles and Responsibilities\\nR...  \n",
       "6  Job description\\nGreetings from “Altisource bu...  \n",
       "7  Job description\\nHiring for Data Analyst(RE)\\n...  \n",
       "8  Job description\\nHiring for Data Analyst(RE)\\n...  \n",
       "9  Roles and Responsibilities\\nTeam Lead - CDM/PV...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading Driver\n",
    "driver = webdriver.Chrome(r\"C:\\Users\\thunderbolt\\sof\\chromedriver.exe\")\n",
    "\n",
    "#Giving name of the website we have to  scrap\n",
    "url = \"https://www.naukri.com/\"\n",
    "\n",
    "# Getting Url\n",
    "driver.get(url)\n",
    "\n",
    "Job_Title = []\n",
    "Location = []\n",
    "Company_Name = []\n",
    "Full_Job_Description = []\n",
    "\n",
    "time.sleep(4)\n",
    "\n",
    "driver.switch_to_window(driver.window_handles[1])\n",
    "driver.close()\n",
    "time.sleep(2)\n",
    "driver.switch_to_window(driver.window_handles[2])\n",
    "driver.close()\n",
    "time.sleep(4)\n",
    "driver.switch_to_window(driver.window_handles[1])\n",
    "driver.close()\n",
    "driver.switch_to_window(driver.window_handles[0])\n",
    "\n",
    "time.sleep(4)\n",
    "\n",
    "Task1=driver.find_element_by_xpath(\"//div[@class='privacyPolicy']/div//button\")\n",
    "Task1.click()\n",
    "\n",
    "time.sleep(15)\n",
    "\n",
    "Task2 =driver.find_element_by_xpath(\"//span[@class='fr geoLocBtn later']\")\n",
    "Task2.click()\n",
    "\n",
    "time.sleep(5)\n",
    "\n",
    "Task3=driver.find_element_by_xpath(\"//div[@class='inpWrap']/input\")\n",
    "Task3.send_keys('Data Analyst')\n",
    "\n",
    "time.sleep(5)\n",
    "\n",
    "Task4= driver.find_element_by_xpath(\"//div[2][@class='suggestor-location']//div[@class='sWrap']//input\")\n",
    "Task4.send_keys('Bangalore')\n",
    "\n",
    "time.sleep(2)\n",
    "\n",
    "Task5= driver.find_element_by_xpath(\"//div[@class='qsb-main-content row']//button\")\n",
    "Task5.click()\n",
    "\n",
    "time.sleep(4)\n",
    "\n",
    "Task6 = driver.find_elements_by_xpath(\"//div[@class='jobTupleHeader']/div/a\")\n",
    "\n",
    "#Getting Job title data\n",
    "for qw in Task6:\n",
    "    we  = qw.text\n",
    "    Job_Title.append(we)\n",
    "    \n",
    "# Putting Data in dataframe   \n",
    "Job_Title = pd.DataFrame(Job_Title)\n",
    "Job_Title = Job_Title.rename({ 0 : 'Job Title'} , axis = 1 )\n",
    "\n",
    "\n",
    "time.sleep(5)\n",
    "\n",
    "# Getting company name data \n",
    "Task7 = driver.find_elements_by_xpath(\"//div[@class='info fleft']//a[@class='subTitle ellipsis fleft']\")\n",
    "time.sleep(5)\n",
    "for rt in Task7:\n",
    "    yu = rt.text\n",
    "    Company_Name.append(yu)\n",
    "\n",
    "#Putting Data in DataFrame\n",
    "Company_Name = pd.DataFrame(Company_Name)\n",
    "Company_Name= Company_Name.rename({0 : 'Company Name'}, axis=1  )\n",
    "\n",
    "time.sleep(3)\n",
    "\n",
    "# Getting Location Data\n",
    "Task8 = driver.find_elements_by_xpath(\"//ul[@class='mt-7']//li[@class='fleft grey-text br2 placeHolderLi location']/span\")\n",
    "for yu in Task8:\n",
    "    ui = yu.text\n",
    "    Location.append(ui)\n",
    "    \n",
    "Location =pd.DataFrame(Location)\n",
    "Location=Location.rename( columns = { 0 : 'Location'} )\n",
    "\n",
    "time.sleep(4)\n",
    "Task9 = driver.find_element_by_xpath(\"//div[@class='content']/section//article/div[@class='jobTupleHeader']//a[@class='title fw500 ellipsis']\")\n",
    "Task9.click()\n",
    "time.sleep(4)\n",
    "Task10 = driver.find_element_by_xpath(\"//div[@class='content']/section//article[2]/div[@class='jobTupleHeader']//a[@class='title fw500 ellipsis']\")\n",
    "Task10.click()\n",
    "time.sleep(4)\n",
    "Task11 = driver.find_element_by_xpath(\"//div[@class='content']/section//article[3]/div[@class='jobTupleHeader']//a[@class='title fw500 ellipsis']\")\n",
    "Task11.click()\n",
    "time.sleep(4)\n",
    "Task12 = driver.find_element_by_xpath(\"//div[@class='content']/section//article[5]/div[@class='jobTupleHeader']//a[@class='title fw500 ellipsis']\")\n",
    "Task12.click()\n",
    "time.sleep(4)\n",
    "Task13 = driver.find_element_by_xpath(\"//div[@class='content']/section//article[6]/div[@class='jobTupleHeader']//a[@class='title fw500 ellipsis']\")\n",
    "Task13.click()\n",
    "time.sleep(4)\n",
    "Task14 = driver.find_element_by_xpath(\"//div[@class='content']/section//article[7]/div[@class='jobTupleHeader']//a[@class='title fw500 ellipsis']\")\n",
    "Task14.click()\n",
    "time.sleep(4)\n",
    "Task15 = driver.find_element_by_xpath(\"//div[@class='content']/section//article[8]/div[@class='jobTupleHeader']//a[@class='title fw500 ellipsis']\")\n",
    "Task15.click()\n",
    "time.sleep(4)\n",
    "Task16 = driver.find_element_by_xpath(\"//div[@class='content']/section//article[9]/div[@class='jobTupleHeader']//a[@class='title fw500 ellipsis']\")\n",
    "Task16.click()\n",
    "time.sleep(4)\n",
    "Task17 = driver.find_element_by_xpath(\"//div[@class='content']/section//article[10]/div[@class='jobTupleHeader']//a[@class='title fw500 ellipsis']\")\n",
    "Task16.click()\n",
    "time.sleep(4)\n",
    "Task18 = driver.find_element_by_xpath(\"//div[@class='content']/section//article[11]/div[@class='jobTupleHeader']//a[@class='title fw500 ellipsis']\")\n",
    "Task18.click()\n",
    "\n",
    "driver.switch_to_window(driver.window_handles[10])\n",
    "#Getting Description\n",
    "Task20 = driver.find_elements_by_xpath(\"//div[@class='jd-container']//section[@class='job-desc']\")\n",
    "Desc1 =[]\n",
    "for hj in Task20:\n",
    "    ko =hj.text\n",
    "    Desc1.append(ko)\n",
    "    \n",
    "desc1=pd.DataFrame(Desc1)\n",
    "time.sleep(4)\n",
    "\n",
    "#Getting Description\n",
    "driver.switch_to_window(driver.window_handles[9])\n",
    "\n",
    "Desc2 =[]\n",
    "Desc3 = []\n",
    "Desc4 = []\n",
    "Desc5 = []\n",
    "Desc6 = []\n",
    "Desc7 = []\n",
    "Desc8 = []\n",
    "Desc9 = []\n",
    "Desc10 = []\n",
    "\n",
    "# Getting Description\n",
    "Task21 = driver.find_elements_by_xpath(\"//div[@class='jd-container']//section[@class='job-desc']\")\n",
    "for kl in Task21:\n",
    "    zx = kl.text\n",
    "    Desc2.append(zx)\n",
    "desc2=pd.DataFrame(Desc2)\n",
    "\n",
    "\n",
    "driver.switch_to_window(driver.window_handles[8])\n",
    "time.sleep(4)\n",
    "\n",
    "# Getting job Description\n",
    "Task22 = driver.find_elements_by_xpath(\"//div[@class='entry-content-wrapper clearfix']//div[@class='clearboth description']\")\n",
    "for cv in Task22:\n",
    "    bn= cv.text\n",
    "    Desc3.append(bn)\n",
    "\n",
    "desc3 = pd.DataFrame(Desc3)\n",
    "\n",
    "driver.switch_to_window(driver.window_handles[7])\n",
    "time.sleep(4)\n",
    "\n",
    "Task23 = driver.find_elements_by_xpath(\"//div[@class='jd-container']//section[@class='job-desc']\")\n",
    "for mn in Task23:\n",
    "    qe = mn.text\n",
    "    Desc4.append(qe)\n",
    "    \n",
    "desc4 = pd.DataFrame(Desc4)\n",
    "\n",
    "time.sleep(4)\n",
    "\n",
    "driver.switch_to_window(driver.window_handles[6])\n",
    "Task24= driver.find_elements_by_xpath(\"//div[@class='jd-container']//section[@class='job-desc']\" )\n",
    "\n",
    "for wr in Task24:\n",
    "    et = wr.text\n",
    "    Desc5.append(et)\n",
    "\n",
    "desc5=pd.DataFrame(Desc5)\n",
    "\n",
    "time.sleep(4)\n",
    "driver.switch_to_window(driver.window_handles[5])\n",
    "\n",
    "#Getting Job Description\n",
    "Task25 =driver.find_elements_by_xpath(\"//div[@class='jd-container']//section[@class='job-desc']\")\n",
    "for yi in Task25:\n",
    "    uo = yi.text\n",
    "    Desc6.append(uo)\n",
    "    \n",
    "desc6 = pd.DataFrame(Desc6)\n",
    "\n",
    "time.sleep(4)\n",
    "\n",
    "driver.switch_to_window(driver.window_handles[4])\n",
    "\n",
    "Task26 = driver.find_elements_by_xpath(\"//section[@class='job-desc']\")\n",
    "for ipk in Task26:\n",
    "    adj = ipk.text\n",
    "    Desc7.append(adj)\n",
    "    \n",
    "desc7= pd.DataFrame(Desc7)\n",
    "\n",
    "#Getting Job Description\n",
    "\n",
    "driver.switch_to_window(driver.window_handles[3])\n",
    "\n",
    "Task27 = driver.find_elements_by_xpath(\"//div[@class='jd-container']//section[@class='job-desc']\")\n",
    "for ihl in Task27:\n",
    "    adjd = ihl.text\n",
    "    Desc8.append(adjd)\n",
    "    \n",
    "desc8 = pd.DataFrame(Desc8)\n",
    "\n",
    "#Getting Job Description\n",
    "\n",
    "driver.switch_to_window(driver.window_handles[2])\n",
    "\n",
    "Task28 = driver.find_elements_by_xpath(\"//div[@class='jd-container']//section[@class='job-desc']\")\n",
    "for ihlk in Task28:\n",
    "    adjds = ihlk.text\n",
    "    Desc9.append(adjds)\n",
    "    \n",
    "desc9 = pd.DataFrame(Desc9)\n",
    "\n",
    "time.sleep(4)\n",
    "\n",
    "#Getting Job Description\n",
    "\n",
    "driver.switch_to_window(driver.window_handles[1])\n",
    "Task29 = driver.find_elements_by_xpath(\"//div[@class='clearboth description']\")\n",
    "for fk in Task29:\n",
    "    al = fk.text\n",
    "    Desc10.append(al)\n",
    "    \n",
    "desc10 = pd.DataFrame(Desc10)\n",
    "\n",
    "# Getting Complete job descrption data \n",
    "Job_Description = pd.concat([desc1,desc2,desc3,desc4,desc5,desc6,desc7,desc8,desc9,desc10], axis=0)\n",
    "Job_Description.index=range(10)\n",
    "Job_Description=Job_Description.rename(columns ={ 0 : 'Job Description'})\n",
    "\n",
    "Job_Title1 = Job_Title.iloc[0:10]\n",
    "Company_Name1 =Company_Name.iloc[0:10]\n",
    "Location1 = Location.iloc[0:10]\n",
    "\n",
    "# Getting complete data \n",
    "Naukri_2= pd.concat([Job_Title1,Company_Name1,Location1,Job_Description], axis=1)\n",
    "Naukri_2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - In this question you have to scrape data using the filters available on the webpage as shown below:\n",
    "        \n",
    "    You have to use the location and salary filter.\n",
    "    You have to scrape data for “Data Scientist” designation for first 10 job results.\n",
    "    You have to scrape the job-title, job-location, company_name, experience_required.\n",
    "    The location filter to be used is “Delhi/NCR”\n",
    "    The salary filter to be used is “3-6” lakhs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job Title</th>\n",
       "      <th>Company Name</th>\n",
       "      <th>Location</th>\n",
       "      <th>Experience</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Hk solutions</td>\n",
       "      <td>Chennai, Delhi NCR, Bengaluru</td>\n",
       "      <td>0-3 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Netomi</td>\n",
       "      <td>Gurgaon</td>\n",
       "      <td>0-1 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Spectral Consultants</td>\n",
       "      <td>Gurgaon</td>\n",
       "      <td>3-6 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Only Fresher / Data Scientist / Data Analyst /...</td>\n",
       "      <td>GABA Consultancy services</td>\n",
       "      <td>Delhi NCR, Ghaziabad, Gurgaon</td>\n",
       "      <td>0-0 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Only Fresher / Data Scientist / Data Analyst /...</td>\n",
       "      <td>GABA Consultancy services</td>\n",
       "      <td>Faridabad, Delhi NCR, Greater Noida</td>\n",
       "      <td>0-0 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>MIS Executive / Data Analyst / Research Analyt...</td>\n",
       "      <td>GABA Consultancy services</td>\n",
       "      <td>Delhi NCR, Greater Noida, Gurgaon</td>\n",
       "      <td>0-5 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Data Analyst / Business Analytics / Fresher An...</td>\n",
       "      <td>GABA Consultancy services</td>\n",
       "      <td>Delhi NCR, Noida, Gurgaon</td>\n",
       "      <td>0-5 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Snaphunt</td>\n",
       "      <td>Delhi NCR</td>\n",
       "      <td>2-5 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Apex Sourcing Info Services Pvt. Ltd.</td>\n",
       "      <td>Delhi NCR</td>\n",
       "      <td>1-3 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Data Analyst - Protiviti Gurgaon</td>\n",
       "      <td>Protiviti India Member Private limited</td>\n",
       "      <td>Gurgaon</td>\n",
       "      <td>1-6 Yrs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Job Title  \\\n",
       "0                                       Data Analyst   \n",
       "1                                       Data Analyst   \n",
       "2                                       Data Analyst   \n",
       "3  Only Fresher / Data Scientist / Data Analyst /...   \n",
       "4  Only Fresher / Data Scientist / Data Analyst /...   \n",
       "5  MIS Executive / Data Analyst / Research Analyt...   \n",
       "6  Data Analyst / Business Analytics / Fresher An...   \n",
       "7                                       Data Analyst   \n",
       "8                                       Data Analyst   \n",
       "9                   Data Analyst - Protiviti Gurgaon   \n",
       "\n",
       "                             Company Name  \\\n",
       "0                            Hk solutions   \n",
       "1                                  Netomi   \n",
       "2                    Spectral Consultants   \n",
       "3               GABA Consultancy services   \n",
       "4               GABA Consultancy services   \n",
       "5               GABA Consultancy services   \n",
       "6               GABA Consultancy services   \n",
       "7                                Snaphunt   \n",
       "8   Apex Sourcing Info Services Pvt. Ltd.   \n",
       "9  Protiviti India Member Private limited   \n",
       "\n",
       "                              Location Experience  \n",
       "0        Chennai, Delhi NCR, Bengaluru    0-3 Yrs  \n",
       "1                              Gurgaon    0-1 Yrs  \n",
       "2                              Gurgaon    3-6 Yrs  \n",
       "3        Delhi NCR, Ghaziabad, Gurgaon    0-0 Yrs  \n",
       "4  Faridabad, Delhi NCR, Greater Noida    0-0 Yrs  \n",
       "5    Delhi NCR, Greater Noida, Gurgaon    0-5 Yrs  \n",
       "6            Delhi NCR, Noida, Gurgaon    0-5 Yrs  \n",
       "7                            Delhi NCR    2-5 Yrs  \n",
       "8                            Delhi NCR    1-3 Yrs  \n",
       "9                              Gurgaon    1-6 Yrs  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing Libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from time import sleep\n",
    "import time \n",
    "import string\n",
    "from random import randint\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "# Loading Driver\n",
    "driver = webdriver.Chrome(r\"C:\\Users\\thunderbolt\\sof\\chromedriver.exe\")\n",
    "\n",
    "#Giving name of the website we have to  scrap\n",
    "url = \"https://www.naukri.com/\"\n",
    "\n",
    "# Getting Url\n",
    "driver.get(url)\n",
    "\n",
    "Job_Title = []\n",
    "Location = []\n",
    "Company_Name = []\n",
    "Experience_Required = []\n",
    "\n",
    "time.sleep(4)\n",
    "\n",
    "driver.switch_to_window(driver.window_handles[1])\n",
    "driver.close()\n",
    "time.sleep(2)\n",
    "driver.switch_to_window(driver.window_handles[2])\n",
    "driver.close()\n",
    "time.sleep(4)\n",
    "driver.switch_to_window(driver.window_handles[1])\n",
    "driver.close()\n",
    "\n",
    "time.sleep(4)\n",
    "\n",
    "driver.switch_to_window(driver.window_handles[0])\n",
    "\n",
    "time.sleep(4)\n",
    "\n",
    "Task1=driver.find_element_by_xpath(\"//div[@class='privacyPolicy']/div//button\")\n",
    "Task1.click()\n",
    "\n",
    "time.sleep(20)\n",
    "\n",
    "Task2 =driver.find_element_by_xpath(\"//span[@class='fr geoLocBtn later']\")\n",
    "Task2.click()\n",
    "\n",
    "time.sleep(5)\n",
    "\n",
    "Task3=driver.find_element_by_xpath(\"//div[@class='inpWrap']/input\")\n",
    "Task3.send_keys('Data Analyst')\n",
    "\n",
    "time.sleep(5)\n",
    "Task5= driver.find_element_by_xpath(\"//div[@class='qsb-main-content row']//button\")\n",
    "Task5.click()\n",
    "\n",
    "time.sleep(5)\n",
    "\n",
    "Task6= driver.find_element_by_xpath(\"//div[@class='filters']/div[2]/div[2]/div[2]/label/i\")\n",
    "Task6.click()\n",
    "\n",
    "time.sleep(10)\n",
    "Task7= driver.find_element_by_xpath(\"//div[@class='filters']/div[3]/div[2]/div[2]/label/i\")\n",
    "Task7.click()\n",
    "\n",
    "time.sleep(5)\n",
    "\n",
    "#Getting Job Title data \n",
    "Task8 = driver.find_elements_by_xpath(\"//div[@class='jobTupleHeader']/div/a\")\n",
    "for qwa in Task8:\n",
    "    weq  = qwa.text\n",
    "    Job_Title.append(weq)\n",
    "    \n",
    "Job_Title = pd.DataFrame(Job_Title)\n",
    "Job_Title = Job_Title.rename({ 0 : 'Job Title'} , axis = 1 )\n",
    "\n",
    "\n",
    "time.sleep(5)\n",
    "\n",
    "# Getting company name \n",
    "Task9 = driver.find_elements_by_xpath(\"//div[@class='mt-7 companyInfo subheading lh16']/a[1]\")\n",
    "\n",
    "for rt in Task9:\n",
    "    yu = rt.text\n",
    "    Company_Name.append(yu)\n",
    "Company_Name\n",
    "\n",
    "Company_Name = pd.DataFrame(Company_Name)\n",
    "Company_Name= Company_Name.rename({0 : 'Company Name'}, axis=1  )\n",
    "\n",
    "time.sleep(5)\n",
    "\n",
    "# Getting Location Data\n",
    "Task10 = driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi location']//span\")\n",
    "for yua in Task10:\n",
    "    uia = yua.text\n",
    "    Location.append(uia)\n",
    "Location[0:5]\n",
    "\n",
    "Location =pd.DataFrame(Location)\n",
    "Location=Location.rename( columns = { 0 : 'Location'} )\n",
    "\n",
    "time.sleep(5)\n",
    "\n",
    "# Getting Experience required data \n",
    "Task11= driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi experience']//span\")\n",
    "for uix in Task11:\n",
    "    opx = uix.text\n",
    "    Experience_Required.append(opx)\n",
    "    \n",
    "Experience_Required = pd.DataFrame(Experience_Required)\n",
    "Experience_Required  = Experience_Required.rename(columns = { 0 : 'Experience'})\n",
    "\n",
    "\n",
    "# Getting Final Data \n",
    "Naukri_Jobs = pd.concat([Job_Title,Company_Name,Location ,Experience_Required ] ,axis=1)\n",
    "Naukri_Jobs3 =Naukri_Jobs.iloc[0:10]\n",
    "Naukri_Jobs3 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4:-  Write a python program to scrape data for first 10 job results for Data scientist Designation in Noida location from glassdoor website. \n",
    " You have to scrape company_name, No. of days ago when job was posted, Rating of the company."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Company Name</th>\n",
       "      <th>Days Posted</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>IBM</td>\n",
       "      <td>24h</td>\n",
       "      <td>3.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ericsson-Worldwide</td>\n",
       "      <td>3d</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>American Express Global Business Travel</td>\n",
       "      <td>22d</td>\n",
       "      <td>3.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Biz2Credit Inc</td>\n",
       "      <td>24d</td>\n",
       "      <td>3.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Brickred</td>\n",
       "      <td>7d</td>\n",
       "      <td>3.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Algoscale</td>\n",
       "      <td>15d</td>\n",
       "      <td>3.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>SearchUrCollege</td>\n",
       "      <td>15d</td>\n",
       "      <td>4.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Healtheoz India</td>\n",
       "      <td>15d</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Techlive</td>\n",
       "      <td>22d</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>xtLytics</td>\n",
       "      <td>2d</td>\n",
       "      <td>3.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              Company Name Days Posted Rating\n",
       "0                                      IBM         24h    3.9\n",
       "1                       Ericsson-Worldwide          3d      4\n",
       "2  American Express Global Business Travel         22d    3.7\n",
       "3                           Biz2Credit Inc         24d    3.7\n",
       "4                                 Brickred          7d    3.7\n",
       "5                                Algoscale         15d    3.4\n",
       "6                          SearchUrCollege         15d    4.8\n",
       "7                          Healtheoz India         15d      5\n",
       "8                                 Techlive         22d      3\n",
       "9                                 xtLytics          2d    3.1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing Libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from time import sleep\n",
    "import time \n",
    "import string\n",
    "from random import randint\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Loading Driver\n",
    "driver = webdriver.Chrome(r\"C:\\Users\\thunderbolt\\sof\\chromedriver.exe\")\n",
    "\n",
    "#Giving name of the website we have to  scrap\n",
    "url = \"https://www.glassdoor.co.in/index.htm\"\n",
    "\n",
    "# Getting Url\n",
    "driver.get(url)\n",
    "\n",
    "\n",
    "Company_Name = []\n",
    "Job_Posting_Days = []\n",
    "Rating = []\n",
    "\n",
    "Task1=driver.find_element_by_xpath(\"//div[@class='locked-home-sign-in']\")\n",
    "Task1.click()\n",
    "time.sleep(5)\n",
    "Task2=driver.find_element_by_xpath(\"//div[@class='minh-modal d-flex flex-column justify-content-between fullHeight mw-400']/div/div/div[3]//div[1]//input\")\n",
    "Task2.send_keys('koko65316531@gmail.com')\n",
    "time.sleep(5)\n",
    "Task3=driver.find_element_by_xpath(\"//div[@class='minh-modal d-flex flex-column justify-content-between fullHeight mw-400']/div/div/div[3]//div[2]//input\")\n",
    "Task3.send_keys('data9scientist@')\n",
    "time.sleep(5)\n",
    "Task4=driver.find_element_by_xpath(\"//button[@class='gd-ui-button minWidthBtn css-8i7bc2']\")\n",
    "Task4.click()\n",
    "time.sleep(5)\n",
    "Task5 = driver.find_element_by_xpath(\"//div[@class='d-flex col-6 p-0 SearchStyles__searchKeywordContainer']/div//input\")\n",
    "Task5.send_keys('Data Scientist')\n",
    "time.sleep(5)\n",
    "Task6 = driver.find_element_by_xpath(\"//div[@class='ml-xsm col-4 p-0 headerSearchInput SearchStyles__searchBarLocationInput css-1ohf0ui']/div[@class='input-wrapper css-q444d9']/input\")\n",
    "Task6.send_keys('Noida')\n",
    "time.sleep(5)\n",
    "Task7 =driver.find_element_by_xpath(\"//button[@class='gd-ui-button ml-std col-auto css-iixdfr']/span\")\n",
    "Task7.click()\n",
    "time.sleep(5)\n",
    "\n",
    "#Getting company data \n",
    "\n",
    "Task8 = driver.find_elements_by_xpath(\"//div[@class='jobHeader d-flex justify-content-between align-items-start']/a\")\n",
    "Task8\n",
    "for qt in Task8:\n",
    "    af = qt.text\n",
    "    Company_Name.append(af)\n",
    "    \n",
    "Company_Name1= pd.DataFrame(Company_Name)\n",
    "Company_Name1 = Company_Name1.rename(columns = {0 : 'Company Name'})\n",
    "Company_Name1\n",
    "\n",
    "# Getting No of Days job posted data \n",
    "Task8 = driver.find_elements_by_xpath(\"//div[@class='d-flex justify-content-between css-1qtdns2']//div[@class='d-flex align-items-end pl-std css-mi55ob']\")  if driver.find_elements_by_xpath(\"//div[@class='d-flex justify-content-between css-1qtdns2']//div[@class='d-flex align-items-end pl-std css-mi55ob']\") else '-' \n",
    "Task8\n",
    "\n",
    "for dk in Task8:\n",
    "    al = dk.text\n",
    "    Job_Posting_Days.append(al)\n",
    "    \n",
    "Job_Posting_Days1 = pd.DataFrame(Job_Posting_Days)\n",
    "Days_Posted =Job_Posting_Days1.rename(columns = { 0 : 'Days Posted'})\n",
    "\n",
    "# Getting company rating data \n",
    "Task9 = driver.find_elements_by_xpath(\"//span[@class='compactStars ']\")\n",
    "\n",
    "for jb in Task9:\n",
    "    al = jb.text\n",
    "    Rating.append(al)\n",
    "    \n",
    "Rating1 = pd.DataFrame(Rating)\n",
    "Rating2=Rating1.rename(columns  = {0 : 'Rating'} )\n",
    "\n",
    "Company_Name2 = Company_Name1.iloc[0:10]\n",
    "Days_Posted1 = Days_Posted.iloc[0:10]\n",
    "Rating3 = Rating2.iloc[0:10]\n",
    "\n",
    "# Getting complete Data scientist job Data From glassdoor.\n",
    "Data_Scientist_Glassdoor  = pd.concat([Company_Name2 ,Days_Posted1 , Rating3] , axis=1)\n",
    "Data_Scientist_Glassdoor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5:- Write a python program to scrape the salary data for Data Scientist designation in Noida location.\n",
    "    \n",
    "You have to scrape Company name, Number of salaries, Average salary, Min salary, Max Salary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Company Name</th>\n",
       "      <th>No Of Salaries</th>\n",
       "      <th>Average Salary</th>\n",
       "      <th>Minimum Salary</th>\n",
       "      <th>Maximum Salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Delhivery</td>\n",
       "      <td>11 salaries</td>\n",
       "      <td>₹ 13,18,563</td>\n",
       "      <td>₹706K</td>\n",
       "      <td>₹11,513K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Accenture</td>\n",
       "      <td>8 salaries</td>\n",
       "      <td>₹ 9,85,497</td>\n",
       "      <td>₹572K</td>\n",
       "      <td>₹1,300K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IBM</td>\n",
       "      <td>7 salaries</td>\n",
       "      <td>₹ 7,53,602</td>\n",
       "      <td>₹581K</td>\n",
       "      <td>₹2,704K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>UnitedHealth Group</td>\n",
       "      <td>7 salaries</td>\n",
       "      <td>₹ 13,23,634</td>\n",
       "      <td>₹710K</td>\n",
       "      <td>₹1,559K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Cognizant Technology Solutions</td>\n",
       "      <td>6 salaries</td>\n",
       "      <td>₹ 9,97,979</td>\n",
       "      <td>₹785K</td>\n",
       "      <td>₹1,251K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Valiance Solutions</td>\n",
       "      <td>6 salaries</td>\n",
       "      <td>₹ 7,72,507</td>\n",
       "      <td>₹497K</td>\n",
       "      <td>₹1,140K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Vidooly Media Tech</td>\n",
       "      <td>6 salaries</td>\n",
       "      <td>₹ 12,689</td>\n",
       "      <td>₹8K</td>\n",
       "      <td>₹20K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Analytics Vidhya</td>\n",
       "      <td>6 salaries</td>\n",
       "      <td>₹ 21,215</td>\n",
       "      <td>₹14K</td>\n",
       "      <td>₹22K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Tata Consultancy Services</td>\n",
       "      <td>5 salaries</td>\n",
       "      <td>₹ 6,77,498</td>\n",
       "      <td>₹480K</td>\n",
       "      <td>₹1,000K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Ericsson-Worldwide</td>\n",
       "      <td>5 salaries</td>\n",
       "      <td>₹ 7,34,456</td>\n",
       "      <td>₹460K</td>\n",
       "      <td>₹1,598K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Company Name No Of Salaries Average Salary  \\\n",
       "0                       Delhivery    11 salaries    ₹ 13,18,563   \n",
       "1                       Accenture     8 salaries     ₹ 9,85,497   \n",
       "2                             IBM     7 salaries     ₹ 7,53,602   \n",
       "3              UnitedHealth Group     7 salaries    ₹ 13,23,634   \n",
       "4  Cognizant Technology Solutions     6 salaries     ₹ 9,97,979   \n",
       "5              Valiance Solutions     6 salaries     ₹ 7,72,507   \n",
       "6              Vidooly Media Tech     6 salaries       ₹ 12,689   \n",
       "7                Analytics Vidhya     6 salaries       ₹ 21,215   \n",
       "8       Tata Consultancy Services     5 salaries     ₹ 6,77,498   \n",
       "9              Ericsson-Worldwide     5 salaries     ₹ 7,34,456   \n",
       "\n",
       "  Minimum Salary Maximum Salary  \n",
       "0          ₹706K       ₹11,513K  \n",
       "1          ₹572K        ₹1,300K  \n",
       "2          ₹581K        ₹2,704K  \n",
       "3          ₹710K        ₹1,559K  \n",
       "4          ₹785K        ₹1,251K  \n",
       "5          ₹497K        ₹1,140K  \n",
       "6            ₹8K           ₹20K  \n",
       "7           ₹14K           ₹22K  \n",
       "8          ₹480K        ₹1,000K  \n",
       "9          ₹460K        ₹1,598K  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing Libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from time import sleep\n",
    "import time \n",
    "import string\n",
    "from random import randint\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "# Loading Driver\n",
    "driver = webdriver.Chrome(r\"C:\\Users\\thunderbolt\\sof\\chromedriver.exe\")\n",
    "\n",
    "#Giving name of the website we have to  scrap\n",
    "url = \"https://www.glassdoor.co.in/Salaries/index.htm\"\n",
    "\n",
    "# Getting Url\n",
    "driver.get(url)\n",
    "\n",
    "Company_Name = []\n",
    "No_Of_Salaries = []\n",
    "Average_salary = []\n",
    "Min_Salary = []\n",
    "Max_Salary = []\n",
    "\n",
    "time.sleep(5)\n",
    "Task1= driver.find_element_by_xpath(\"//div[@class='search-bar minimized']/div//input[@class='keyword']\")\n",
    "Task1.send_keys(\"Data Scientist\")\n",
    "time.sleep(3)\n",
    "Task2 =driver.find_element_by_xpath(\"//div[@class='search-bar minimized']/div//input[@class='loc']\")\n",
    "Task2.send_keys(\"Noida\")\n",
    "time.sleep(3)\n",
    "Task3 = driver.find_element_by_xpath(\"//div[@class='search-bar minimized']//button\")\n",
    "Task3.click()\n",
    "time.sleep(5)\n",
    "\n",
    "# Getting Company Name Data \n",
    "\n",
    "Task4 = driver.find_elements_by_xpath(\"//div[@class='col-md-6']/div/div[2]//p[2]\")\n",
    "for qo in Task4:\n",
    "    al = qo.text\n",
    "    Company_Name.append(al)\n",
    "    \n",
    "Company_Name1= pd.DataFrame(Company_Name)\n",
    "Company_Name1= Company_Name1.rename({0 : 'Company Name'},axis =1)\n",
    "\n",
    "# Getting No of salaries data \n",
    "Task5 = driver.find_elements_by_xpath(\"//div[@class='col-md-6']/div/div[2]//p[5]\")\n",
    "for qm in Task5:\n",
    "    zp = qm.text\n",
    "    No_Of_Salaries.append(zp)\n",
    "    \n",
    "No_Of_Salaries1 = pd.DataFrame(No_Of_Salaries)\n",
    "No_Of_Salaries1 = No_Of_Salaries1.rename({0 : 'No Of Salaries'} , axis =1)\n",
    "\n",
    "# Getting Average Salary\n",
    "Task6 = driver.find_elements_by_xpath(\"//div[@class='col-2 d-none d-md-flex flex-row justify-content-end']/strong\")\n",
    "for wb in Task6:\n",
    "    ag = wb.text\n",
    "    Average_salary.append(ag)\n",
    "\n",
    "Average_salary1 = pd.DataFrame(Average_salary)\n",
    "Average_salary1  = Average_salary1 .rename({ 0 : 'Average Salary'}, axis=1)\n",
    "\n",
    "# Getting Minimum Salary Data \n",
    "\n",
    "Task7 = driver.find_elements_by_xpath(\"//div[@class='common__RangeBarStyle__values common__flex__justifySpaceBetween common__flex__container ']/span[1]\")\n",
    "for vq in Task7:\n",
    "    ap = vq.text\n",
    "    Min_Salary.append(ap)\n",
    "    \n",
    "Min_Salary1 = pd.DataFrame(Min_Salary)\n",
    "Min_Salary1 = Min_Salary1.rename({ 0 : 'Minimum Salary'},  axis=1)\n",
    "\n",
    "# Getting Maximum salary data \n",
    "Task8 = driver.find_elements_by_xpath(\"//div[@class='common__RangeBarStyle__values common__flex__justifySpaceBetween common__flex__container ']/span[2]\")\n",
    "for qv in Task8:\n",
    "    ze = qv.text\n",
    "    Max_Salary.append(ze)\n",
    "    \n",
    "Max_Salary1 = pd.DataFrame(Max_Salary)\n",
    "Max_Salary1 = Max_Salary1.rename({0 : 'Maximum Salary'}, axis=1)\n",
    "\n",
    "Data_Scientist_Glassdoor_Salaries = pd.concat([Company_Name1,No_Of_Salaries1,Average_salary1 ,Min_Salary1, Max_Salary1], axis=1)\n",
    "\n",
    "# Getting Final Data\n",
    "\n",
    "Data_Scientist_Glassdoor_Salaries  = Data_Scientist_Glassdoor_Salaries .iloc[0:10]\n",
    "Data_Scientist_Glassdoor_Salaries "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 - Shoes Data Scraping from my Myntra.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brand</th>\n",
       "      <th>Shoe Description</th>\n",
       "      <th>Price (Rs)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ADIDAS Originals</td>\n",
       "      <td>Men ZX 750 HD Sneakers</td>\n",
       "      <td>6799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ADIDAS Originals</td>\n",
       "      <td>Women Sleek Super Sneakers</td>\n",
       "      <td>7999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ADIDAS Originals</td>\n",
       "      <td>Men Solid AR Training Shoes</td>\n",
       "      <td>6799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Geox</td>\n",
       "      <td>Men Leather Formal Driving</td>\n",
       "      <td>9490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ADIDAS</td>\n",
       "      <td>Men FluidFlow Running Shoes</td>\n",
       "      <td>7999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Cole Haan</td>\n",
       "      <td>Men Wingtip Oxford Sneakers</td>\n",
       "      <td>12999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>ALDO</td>\n",
       "      <td>Men Textured Leather Loafers</td>\n",
       "      <td>12999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Clarks</td>\n",
       "      <td>Men Leather Formal Loafers</td>\n",
       "      <td>6999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Reebok Classic</td>\n",
       "      <td>Men Royal Complete Clean LX</td>\n",
       "      <td>6999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>Geox</td>\n",
       "      <td>Men Leather Formal Derbys</td>\n",
       "      <td>12990</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               Brand              Shoe Description Price (Rs)\n",
       "0   ADIDAS Originals        Men ZX 750 HD Sneakers      6799 \n",
       "1   ADIDAS Originals    Women Sleek Super Sneakers       7999\n",
       "2   ADIDAS Originals   Men Solid AR Training Shoes      6799 \n",
       "3               Geox    Men Leather Formal Driving       9490\n",
       "4             ADIDAS   Men FluidFlow Running Shoes       7999\n",
       "..               ...                           ...        ...\n",
       "45         Cole Haan   Men Wingtip Oxford Sneakers      12999\n",
       "46              ALDO  Men Textured Leather Loafers      12999\n",
       "47            Clarks    Men Leather Formal Loafers       6999\n",
       "48    Reebok Classic   Men Royal Complete Clean LX       6999\n",
       "49              Geox     Men Leather Formal Derbys      12990\n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing Libraries \n",
    "\n",
    "import numpy as np \n",
    "import  pandas as pd\n",
    "from selenium import webdriver\n",
    "from time import  sleep\n",
    "import time\n",
    "import string\n",
    "from random import randint\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Loading driver \n",
    "driver = webdriver.Chrome(r\"C:\\Users\\thunderbolt\\sof\\chromedriver.exe\")\n",
    "\n",
    "#Giving name of the website we have to  scrap\n",
    "url = \"https://www.myntra.com/shoes\"\n",
    "\n",
    "# Getting Url\n",
    "driver.get(url)\n",
    "\n",
    "time.sleep(8)\n",
    "\n",
    "Brand = []\n",
    "Shoe_Description = []\n",
    "Price = []\n",
    "\n",
    "Task1 = driver.find_element_by_xpath(\"//ul[@class='price-list']/li[2]/label[@class='common-customCheckbox vertical-filters-label']//div[@class='common-checkboxIndicator']\")\n",
    "Task1.click()\n",
    "\n",
    "\n",
    "time.sleep(5)\n",
    "\n",
    "Task2 = driver.find_element_by_xpath(\"//ul/li[1][@class='colour-listItem']/label[1]/div\")\n",
    "Task2.click()\n",
    "time.sleep(5)\n",
    "\n",
    "# Getting Brand Data \n",
    "Task3 = driver.find_elements_by_xpath(\"//div[@class='product-productMetaInfo']/h3\")\n",
    "\n",
    "for qmx in Task3:\n",
    "    qzp = qmx.text\n",
    "    Brand.append(qzp)\n",
    "\n",
    "Brand1 = pd.DataFrame(Brand)\n",
    "Brand1 = Brand1.rename({ 0 : 'Brand'} , axis=1)\n",
    "time.sleep(6)\n",
    "\n",
    "# Getting Shoe Description Data\n",
    "Task4 = driver.find_elements_by_xpath(\"//div[@class='product-productMetaInfo']/h4\")\n",
    "\n",
    "for qcp in Task4:\n",
    "    qxp = qcp.text\n",
    "    Shoe_Description.append(qxp)\n",
    "    \n",
    "time.sleep(5)   \n",
    "\n",
    "Shoe_Description1 = pd.DataFrame(Shoe_Description)\n",
    "Shoe_Description2=Shoe_Description1.iloc[0:100:2]\n",
    "Shoe_Description2.index=range(50)\n",
    "Shoe_Description2 = Shoe_Description2.rename({ 0 : 'Shoe Description'}, axis=1)\n",
    "\n",
    "time.sleep(5)   \n",
    "\n",
    "# Getting Shoe Price data \n",
    "\n",
    "Task5 = driver.find_elements_by_xpath(\"//div[@class='product-price']/span[1]\")\n",
    "\n",
    "for qvp in Task5:\n",
    "    ald = qvp.text\n",
    "    Price.append(ald)\n",
    "\n",
    "Price1 = pd.DataFrame(Price)\n",
    "Price1['First'] =Price1[0].str.split(' ').str[0]\n",
    "Price1['Second'] =Price1[0].str.split(' ').str[1]\n",
    "Price1['Last'] =Price1[0].str.split(' ').str[2]\n",
    "Price1['Most Last'] =Price1[0].str.split(' ').str[3]\n",
    "Price1 =Price1.drop([0], axis=1)\n",
    "Price1 =Price1.drop(['Last'], axis=1)\n",
    "Price1 =Price1.drop(['Most Last'], axis=1)\n",
    "Price1['Second'] = Price1['Second'].str.replace( 'Rs.' , ' ' )\n",
    "Price1['Second'] = Price1['Second'].str.replace( '.' , ' ' )\n",
    "Price1= Price1.drop(['First'] , axis= 1)\n",
    "Price2 =Price1.rename(columns = { 'Second' : 'Price (Rs)'})\n",
    "time.sleep(5)   \n",
    "Shoes1 = pd.concat([Brand1 ,Shoe_Description2 , Price2] , axis=1)\n",
    "time.sleep(5)   \n",
    "\n",
    "\n",
    "# Getting Brand Data of next page\n",
    "Task6 = driver.find_element_by_xpath(\"//li[@class='pagination-next']/a\")\n",
    "Task6.click()\n",
    "\n",
    "Brand4 = [] \n",
    "Shoe_Description4 = [] \n",
    "Price4 = []\n",
    "\n",
    "time.sleep(5)\n",
    "Task7 = driver.find_elements_by_xpath(\"//div[@class='product-productMetaInfo']/h3\")\n",
    "for qvl in Task7:\n",
    "    adp = qvl.text\n",
    "    Brand4.append(adp)\n",
    "    \n",
    "time.sleep(5)\n",
    "\n",
    "Brand4 = pd.DataFrame(Brand4)\n",
    "Brand4 = Brand4.rename({0:'Brand'}, axis =1)\n",
    "\n",
    "time.sleep(5)   \n",
    "\n",
    "# Getting Brand Description of next page\n",
    "\n",
    "Task8 = driver.find_elements_by_xpath(\"//div[@class='product-productMetaInfo']/h4\")\n",
    "for qcpv in Task8:\n",
    "    qxpv = qcpv.text\n",
    "    Shoe_Description4.append(qxpv)\n",
    "    \n",
    "Shoe_Description4 = pd.DataFrame(Shoe_Description4)\n",
    "Shoe_Description4=Shoe_Description4.iloc[0:100:2]\n",
    "Shoe_Description4.index=range(50)\n",
    "Shoe_Description4 = Shoe_Description4.rename({ 0 : 'Shoe Description'}, axis=1)\n",
    "time.sleep(5) \n",
    "\n",
    "#Getting Price Data from next page\n",
    "Task9 = driver.find_elements_by_xpath(\"//div[@class='product-price']/span[1]\")\n",
    "for qvpz in Task9:\n",
    "    aldz = qvpz.text\n",
    "    Price4.append(aldz)\n",
    "Price4 = pd.DataFrame(Price4)\n",
    "\n",
    "Price4['First'] =Price4[0].str.split(' ').str[0]\n",
    "Price4['Second'] =Price4[0].str.split(' ').str[1]\n",
    "Price4['Last'] =Price4[0].str.split(' ').str[2]\n",
    "Price4['Most Last'] =Price4[0].str.split(' ').str[3]\n",
    "\n",
    "Price4=Price4.drop([0], axis=1)\n",
    "Price4 =Price4.drop(['Last'], axis=1)\n",
    "Price4 =Price4.drop(['Most Last'], axis=1)\n",
    "Price4['Second'] = Price4['Second'].str.replace( 'Rs.' , ' ' )\n",
    "Price4['Second'] = Price4['Second'].str.replace( '.' , ' ' )\n",
    "Price4= Price4.drop(['First'] , axis= 1)\n",
    "Price5 =Price4.rename(columns = { 'Second' : 'Price (Rs)'})\n",
    "time.sleep(5) \n",
    "\n",
    "Shoes2 = pd.concat([Brand4 ,Shoe_Description4, Price5] , axis=1)\n",
    "\n",
    "Shoes = pd.concat([Shoes1 , Shoes2] , axis =0)\n",
    "\n",
    "# Getting Final Shoes Data\n",
    "Shoes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7 -  Scrape data for first 100 sneakers you find when you visit flipkart.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brand</th>\n",
       "      <th>Product Description</th>\n",
       "      <th>Price</th>\n",
       "      <th>Discount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WROGN</td>\n",
       "      <td>Sneakers For Men</td>\n",
       "      <td>₹1,349</td>\n",
       "      <td>50% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Kraasa</td>\n",
       "      <td>Sneakers For Men</td>\n",
       "      <td>₹299</td>\n",
       "      <td>70% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Chevit</td>\n",
       "      <td>Combo Pack of 4 Casual Sneakers With Sneakers ...</td>\n",
       "      <td>₹461</td>\n",
       "      <td>76% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Chevit</td>\n",
       "      <td>171 Smart Tan Lace-Ups Casuals for Men Sneaker...</td>\n",
       "      <td>₹236</td>\n",
       "      <td>52% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Chevit</td>\n",
       "      <td>Chevit Latest Fashion Combo Pack of 2 Pairs Ca...</td>\n",
       "      <td>₹525</td>\n",
       "      <td>64% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Bonexy</td>\n",
       "      <td>Sneakers For Men</td>\n",
       "      <td>₹499</td>\n",
       "      <td>50% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Bersache</td>\n",
       "      <td>Bersache Sports (Walking &amp; Gym Shoes) Running,...</td>\n",
       "      <td>₹498</td>\n",
       "      <td>50% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Essence</td>\n",
       "      <td>Sneakers For Men</td>\n",
       "      <td>₹374</td>\n",
       "      <td>62% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Chevit</td>\n",
       "      <td>Combo Pack of 5 Casual Sneakers With Sneakers ...</td>\n",
       "      <td>₹599</td>\n",
       "      <td>75% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>BRUTON</td>\n",
       "      <td>Men Combo Pack of 3 Casual Sneakers For Men</td>\n",
       "      <td>₹649</td>\n",
       "      <td>78% off</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Brand                                Product Description   Price  \\\n",
       "0      WROGN                                   Sneakers For Men  ₹1,349   \n",
       "1     Kraasa                                   Sneakers For Men    ₹299   \n",
       "2     Chevit  Combo Pack of 4 Casual Sneakers With Sneakers ...    ₹461   \n",
       "3     Chevit  171 Smart Tan Lace-Ups Casuals for Men Sneaker...    ₹236   \n",
       "4     Chevit  Chevit Latest Fashion Combo Pack of 2 Pairs Ca...    ₹525   \n",
       "..       ...                                                ...     ...   \n",
       "15    Bonexy                                   Sneakers For Men    ₹499   \n",
       "16  Bersache  Bersache Sports (Walking & Gym Shoes) Running,...    ₹498   \n",
       "17   Essence                                   Sneakers For Men    ₹374   \n",
       "18    Chevit  Combo Pack of 5 Casual Sneakers With Sneakers ...    ₹599   \n",
       "19    BRUTON        Men Combo Pack of 3 Casual Sneakers For Men    ₹649   \n",
       "\n",
       "   Discount  \n",
       "0   50% off  \n",
       "1   70% off  \n",
       "2   76% off  \n",
       "3   52% off  \n",
       "4   64% off  \n",
       "..      ...  \n",
       "15  50% off  \n",
       "16  50% off  \n",
       "17  62% off  \n",
       "18  75% off  \n",
       "19  78% off  \n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing Libraries \n",
    "\n",
    "import numpy as np \n",
    "import  pandas as pd\n",
    "from selenium import webdriver\n",
    "from time import  sleep\n",
    "import time\n",
    "import string\n",
    "from random import randint\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Loading driver \n",
    "driver = webdriver.Chrome(r\"C:\\Users\\thunderbolt\\sof\\chromedriver.exe\")\n",
    "\n",
    "#Giving name of the website we have to  scrap\n",
    "url = \"https://www.flipkart.com/\"\n",
    "\n",
    "# Getting Url\n",
    "driver.get(url)\n",
    "\n",
    "time.sleep(8)\n",
    "\n",
    "Brand = []\n",
    "Product_Description = []\n",
    "Price = []\n",
    "Discount = []\n",
    "\n",
    "time.sleep(5)\n",
    "Task1 = driver.find_element_by_xpath(\"//div[@class='_3Njdz7']/button\")\n",
    "Task1.click()\n",
    "time.sleep(5)\n",
    "Task2 = driver.find_element_by_xpath(\"//div[@class='O8ZS_U']/input\")\n",
    "Task2.send_keys('Sneakers')\n",
    "time.sleep(5)\n",
    "Task3 = driver.find_element_by_xpath(\"//button[@class='vh79eN']\")\n",
    "Task3.click()\n",
    "time.sleep(5)\n",
    "\n",
    "# Getting Brand data from first page\n",
    "Task4 = driver.find_elements_by_xpath(\"//div[@class='_2B_pmu']\")\n",
    "\n",
    "for qc in Task4:\n",
    "    qz = qc.text\n",
    "    Brand.append(qz)\n",
    "    \n",
    "Brand1 = pd.DataFrame(Brand)\n",
    "Brand1 = Brand1.rename(columns = { 0 : 'Brand'})\n",
    "time.sleep(5)\n",
    "\n",
    "# Getting Product Description data from first page\n",
    "Task5 = driver.find_elements_by_xpath(\"//div[@class='_2LFGJH']/a[1]\")\n",
    "\n",
    "for qm in Task5:\n",
    "    qzp = qm.text\n",
    "    Product_Description.append(qzp)\n",
    "Product_Description1 = pd.DataFrame(Product_Description)\n",
    "Product_Description1 = Product_Description1.rename({ 0: 'Product Description'} , axis=1 )\n",
    "\n",
    "time.sleep(5)\n",
    "\n",
    "# Getting Price data from first page\n",
    "\n",
    "Task6 = driver.find_elements_by_xpath(\"//div[@class='_1uv9Cb']/div[1]\")\n",
    "\n",
    "for apv in Task6:\n",
    "    apx =apv.text\n",
    "    Price.append(apx)\n",
    "    \n",
    "Price1 = pd.DataFrame(Price)\n",
    "Price1 = Price1.rename(columns = { 0 : 'Price'})\n",
    "Price2 = Price1.iloc[0:40]\n",
    "time.sleep(5)\n",
    "\n",
    "# Getting Discount data from first page\n",
    "Task7 = driver.find_elements_by_xpath(\"//div[@class='VGWI6T']/span\")\n",
    "for gzp in Task7:\n",
    "    avq = gzp.text\n",
    "    Discount.append(avq)\n",
    "\n",
    "Discount1 = pd.DataFrame(Discount)\n",
    "Discount1 =Discount1.rename(columns = { 0 : 'Discount'})\n",
    "Sneakers1 =pd.concat([Brand1,Product_Description1,Price2,Discount1] , axis=1)\n",
    "time.sleep(5)\n",
    "Task8 = driver.find_element_by_xpath(\"//a[@class='_3fVaIS']/span\")\n",
    "Task8.click()\n",
    "time.sleep(5)\n",
    "\n",
    "\n",
    "Brand3 = []\n",
    "Product_Description3 = []\n",
    "Price3 = []\n",
    "Discount3 = []\n",
    "\n",
    "\n",
    "\n",
    "# Getting Brand data from second page\n",
    "Task9 = driver.find_elements_by_xpath(\"//div[@class='_2B_pmu']\")\n",
    "\n",
    "for qcx in Task9:\n",
    "    qzx = qcx.text\n",
    "    Brand3.append(qzx)\n",
    "\n",
    "Brand3 = pd.DataFrame(Brand3)\n",
    "Brand3 = Brand3.rename(columns = { 0 : 'Brand'})\n",
    "\n",
    "time.sleep(5)\n",
    "\n",
    "# Getting product description data from second page\n",
    "Task10 = driver.find_elements_by_xpath(\"//div[@class='_2LFGJH']/a[1]\")\n",
    "\n",
    "for qmq in Task10:\n",
    "    qzpq = qmq.text\n",
    "    Product_Description3.append(qzpq)\n",
    "\n",
    "Product_Description3 = pd.DataFrame(Product_Description3)\n",
    "Product_Description3 = Product_Description3.rename({ 0: 'Product Description'} , axis=1 )\n",
    "\n",
    "# Getting Price data from second page\n",
    "time.sleep(5)\n",
    "\n",
    "Task11 = driver.find_elements_by_xpath(\"//div[@class='_1uv9Cb']/div[1]\")\n",
    "\n",
    "for apvq in Task11:\n",
    "    apxq =apvq.text\n",
    "    Price3.append(apxq)\n",
    "\n",
    "Price3 = pd.DataFrame(Price3)\n",
    "Price3 = Price3.rename(columns = { 0 : 'Price'})\n",
    "\n",
    "\n",
    "Price3 = Price3.iloc[0:40]\n",
    "time.sleep(3)\n",
    "\n",
    "# Getting Discount  data from second page\n",
    "Task12 = driver.find_elements_by_xpath(\"//div[@class='VGWI6T']/span\")\n",
    "\n",
    "for gzpz in Task12:\n",
    "    avqz = gzpz.text\n",
    "    Discount3.append(avqz)\n",
    "\n",
    "Discount3 = pd.DataFrame(Discount3)\n",
    "\n",
    "Discount3 =Discount3.rename(columns = { 0 : 'Discount'})\n",
    "time.sleep(2)\n",
    "Sneakers2 =pd.concat([Brand3,Product_Description3,Price3,Discount3] , axis=1)\n",
    "time.sleep(3)\n",
    "Task14 = driver.find_element_by_xpath(\"//nav[@class='_1ypTlJ']/a[3]\")\n",
    "Task14.click()\n",
    "time.sleep(5)\n",
    "\n",
    "Brand4 = []\n",
    "Product_Description4 = []\n",
    "Price4 = []\n",
    "Discount4 = []\n",
    "\n",
    "time.sleep(5)\n",
    "\n",
    "# Getting Brand Data from third page\n",
    "Task15 = driver.find_elements_by_xpath(\"//div[@class='_2B_pmu']\")\n",
    "\n",
    "\n",
    "for qcxq in Task15:\n",
    "    qzxq = qcxq.text\n",
    "    Brand4.append(qzxq)\n",
    "\n",
    "Brand4 = pd.DataFrame(Brand4)\n",
    "Brand4 = Brand4.rename(columns = { 0 : 'Brand'})\n",
    "\n",
    "time.sleep(5)\n",
    "\n",
    "# Getting Product Description  Data from third page\n",
    "\n",
    "Task16 = driver.find_elements_by_xpath(\"//div[@class='_2LFGJH']/a[1]\")\n",
    "\n",
    "for qmqx in Task16:\n",
    "    qzpqx = qmqx.text\n",
    "    Product_Description4.append(qzpqx)\n",
    "\n",
    "Product_Description4 = pd.DataFrame(Product_Description4)\n",
    "Product_Description4 = Product_Description4.rename({ 0: 'Product Description'} , axis=1 )\n",
    "\n",
    "time.sleep(5)\n",
    "\n",
    "# Getting Price data from third page\n",
    "Task17 = driver.find_elements_by_xpath(\"//div[@class='_1uv9Cb']/div[1]\")\n",
    "\n",
    "for apvqg in Task17:\n",
    "    apxqg =apvqg.text\n",
    "    Price4.append(apxqg)\n",
    "\n",
    "Price4 = pd.DataFrame(Price4)\n",
    "Price4 = Price4.rename(columns = { 0 : 'Price'})\n",
    "\n",
    "Price4 = Price4.iloc[0:40]\n",
    "time.sleep(3)\n",
    "\n",
    "# Getting Discount Data from third page\n",
    "Task18 = driver.find_elements_by_xpath(\"//div[@class='VGWI6T']/span\")\n",
    "\n",
    "for gzpzq in Task18:\n",
    "    avqzq = gzpzq.text\n",
    "    Discount4.append(avqzq)\n",
    "\n",
    "Discount4 = pd.DataFrame(Discount4)\n",
    "\n",
    "Discount4 =Discount4.rename(columns = { 0 : 'Discount'})\n",
    "\n",
    "time.sleep(3)\n",
    "Brand6 = Brand4.iloc[0:20]\n",
    "Product_Description6 = Product_Description4.iloc[0:20]\n",
    "Price6 = Price4.iloc[0:20]\n",
    "Discount6 = Discount4.iloc[0:20]\n",
    "time.sleep(3)\n",
    "Sneakers3 =pd.concat([Brand6,Product_Description6,Price6,Discount6] , axis=1)\n",
    "\n",
    "# Getting Final 100 Sneakers Data.\n",
    "Sneakers_Final = pd.concat([Sneakers1,Sneakers2,Sneakers3] ,axis=0)\n",
    "Sneakers_Final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8 - Web scraping of Laptop data from amazon.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "from time import sleep\n",
    "from random import randint\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import time\n",
    "import string\n",
    "\n",
    "# Loading Driver\n",
    "driver = webdriver.Chrome(r\"C:\\Users\\thunderbolt\\sof\\chromedriver.exe\")\n",
    "\n",
    "# Give name of website we have to scrap\n",
    "url=\"https://www.amazon.in/\"\n",
    "\n",
    "driver.get(url)\n",
    "\n",
    "time.sleep(5)\n",
    "\n",
    "Title = []\n",
    "Rating = []\n",
    "Price = []\n",
    "\n",
    "time.sleep(5)\n",
    "\n",
    "# Writing Laptop \n",
    "Task1 = driver.find_element_by_xpath(\"//div//div[@class='nav-search-field ']/input[@id='twotabsearchtextbox']\")\n",
    "Task1.send_keys('Laptop')\n",
    "time.sleep(2)\n",
    "Task2=driver.find_element_by_xpath(\"//div//span[@class='nav-search-submit-text nav-sprite']/input\")\n",
    "Task2.click()\n",
    "\n",
    "time.sleep(2)\n",
    "Task3=driver.find_element_by_xpath(\"//div/ul/li[17]/span/a/div/label/i\")\n",
    "Task3.click()\n",
    "time.sleep(5)\n",
    "Task3=driver.find_element_by_xpath(\"//div/ul/li[17]/span/a/div/label/i\")\n",
    "Task3.click()\n",
    "time.sleep(5)\n",
    "\n",
    "# Getting brand Data \n",
    "Task5=driver.find_elements_by_xpath(\"//span[@class='a-size-medium a-color-base a-text-normal']\")\n",
    "\n",
    "for yu in Task5:\n",
    "    hj=yu.text\n",
    "    Title.append(hj)\n",
    "\n",
    "Title=pd.DataFrame(Title)\n",
    "\n",
    "time.sleep(5)\n",
    "\n",
    "# Getting price Data\n",
    "Task7=driver.find_elements_by_xpath(\"//div[@class='a-row']/div/a/span[@class='a-price']//span[@class='a-price-whole']\") if ( driver.find_elements_by_xpath(\"//div[@class='a-row']/div/a/span[@class='a-price']//span[@class='a-price-whole']\") ) else (' ')\n",
    "\n",
    "for jk in Task7:\n",
    "    sd=jk.text\n",
    "    Price.append(sd)\n",
    "    \n",
    "time.sleep(5)\n",
    "    \n",
    "content = driver.page_source\n",
    "soup = BeautifulSoup(content)\n",
    "rating = soup.find_all('span' , class_='a-icon-alt')\n",
    "Rating5 = []\n",
    "for axc in rating:\n",
    "    Rating5.append(axc.get_text())\n",
    "Rating6 = pd.DataFrame(Rating5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "Title1 =Title.drop([0,1,14,15,28,29] , axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "Title1.index=range(24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Laptop Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dell Alienware m15(R3) 15.6-inch FHD Gaming La...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HP Pavilion x360 Core i7 8th Gen 14-inch Touch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Asus Gaming Laptop ROG Strix G17 i7-10750H(16 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(Renewed) Dell Latitude E7240 12.5-inch Laptop...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(Renewed) HP EliteBook X360 1030 G2 Laptop (Co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(Renewed) HP EliteBook 1030 G2 X 360 Notebook ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(Renewed) Mi Notebook Horizon Edition 14 Intel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Lenovo Legion 5Pi 10th Gen Intel Core i7 15.6\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>HP 14s dr1006TU 14-inch Laptop (10th Gen Core ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>(Renewed) Dell Latitude E7240 12.5-inch Laptop...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>(Renewed) Dell Latitude Laptop E7250 Intel Cor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>MSI Gaming GL65 Leopard , Intel 9th Gen. i7-97...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Dell 2019 Dell Inspiron 14 5482 14 Inch FHD 2-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Dell XPS 9570 15.6-inch FHD Laptop (8th Gen Co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Dell Alienware m15 15.6-inch FHD Gaming Laptop...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Dell Inspiron 7570 15.6-inch UHD 4K with Touch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>(CERTIFIED REFURBISHED) Dell Latitude 7389 13....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>(Renewed) HP Omen 10th Gen Intel Core i7 Proce...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>HP Notebook PC 340S G7 14-inch Laptop (10th Ge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>(Renewed) HP EliteBook 820 G1 Laptop (Core i7 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>(Renewed) Dell XPS 9370 13.3-inch FHD Display ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Laptop Name\n",
       "0   Dell Alienware m15(R3) 15.6-inch FHD Gaming La...\n",
       "1   HP Pavilion x360 Core i7 8th Gen 14-inch Touch...\n",
       "2   Asus Gaming Laptop ROG Strix G17 i7-10750H(16 ...\n",
       "4   (Renewed) Dell Latitude E7240 12.5-inch Laptop...\n",
       "5   (Renewed) HP EliteBook X360 1030 G2 Laptop (Co...\n",
       "7   (Renewed) HP EliteBook 1030 G2 X 360 Notebook ...\n",
       "8   (Renewed) Mi Notebook Horizon Edition 14 Intel...\n",
       "9   Lenovo Legion 5Pi 10th Gen Intel Core i7 15.6\"...\n",
       "11  HP 14s dr1006TU 14-inch Laptop (10th Gen Core ...\n",
       "12  (Renewed) Dell Latitude E7240 12.5-inch Laptop...\n",
       "13  (Renewed) Dell Latitude Laptop E7250 Intel Cor...\n",
       "14  MSI Gaming GL65 Leopard , Intel 9th Gen. i7-97...\n",
       "15  Dell 2019 Dell Inspiron 14 5482 14 Inch FHD 2-...\n",
       "16  Dell XPS 9570 15.6-inch FHD Laptop (8th Gen Co...\n",
       "17  Dell Alienware m15 15.6-inch FHD Gaming Laptop...\n",
       "18  Dell Inspiron 7570 15.6-inch UHD 4K with Touch...\n",
       "19  (CERTIFIED REFURBISHED) Dell Latitude 7389 13....\n",
       "20  (Renewed) HP Omen 10th Gen Intel Core i7 Proce...\n",
       "21  HP Notebook PC 340S G7 14-inch Laptop (10th Ge...\n",
       "22  (Renewed) HP EliteBook 820 G1 Laptop (Core i7 ...\n",
       "23  (Renewed) Dell XPS 9370 13.3-inch FHD Display ..."
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Title2 =Title1.drop([3,6,10] ,axis=0)\n",
    "Title2.rename(columns ={ 0 : 'Laptop Name'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "Title2.index =range(21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Laptop Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dell Alienware m15(R3) 15.6-inch FHD Gaming La...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HP Pavilion x360 Core i7 8th Gen 14-inch Touch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Asus Gaming Laptop ROG Strix G17 i7-10750H(16 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(Renewed) Dell Latitude E7240 12.5-inch Laptop...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(Renewed) HP EliteBook X360 1030 G2 Laptop (Co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(Renewed) HP EliteBook 1030 G2 X 360 Notebook ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(Renewed) Mi Notebook Horizon Edition 14 Intel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Lenovo Legion 5Pi 10th Gen Intel Core i7 15.6\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>HP 14s dr1006TU 14-inch Laptop (10th Gen Core ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(Renewed) Dell Latitude E7240 12.5-inch Laptop...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Laptop Name\n",
       "0  Dell Alienware m15(R3) 15.6-inch FHD Gaming La...\n",
       "1  HP Pavilion x360 Core i7 8th Gen 14-inch Touch...\n",
       "2  Asus Gaming Laptop ROG Strix G17 i7-10750H(16 ...\n",
       "3  (Renewed) Dell Latitude E7240 12.5-inch Laptop...\n",
       "4  (Renewed) HP EliteBook X360 1030 G2 Laptop (Co...\n",
       "5  (Renewed) HP EliteBook 1030 G2 X 360 Notebook ...\n",
       "6  (Renewed) Mi Notebook Horizon Edition 14 Intel...\n",
       "7  Lenovo Legion 5Pi 10th Gen Intel Core i7 15.6\"...\n",
       "8  HP 14s dr1006TU 14-inch Laptop (10th Gen Core ...\n",
       "9  (Renewed) Dell Latitude E7240 12.5-inch Laptop..."
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Title3 = Title2.iloc[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "Title3 =Title3.rename(columns = { 0 : 'Laptop Name'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "Price1 =pd.DataFrame(Price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "Price2 = Price1.drop([0,1,14,15,27,28], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "Price3 = Price2.drop ( [5,8,12] , axis= 0)\n",
    "Price3= Price3.rename(columns = { 0: 'Price'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "Price4=Price3.iloc[0:10]\n",
    "Price4.index=range(10)\n",
    "Rating7 =Rating6.iloc[5:25]\n",
    "Rating7.index=range(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "Rating8 = Rating7.iloc[0:10]\n",
    "Rating8= Rating8.rename(columns ={0 :'Rating'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Laptop Name</th>\n",
       "      <th>Price</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dell Alienware m15(R3) 15.6-inch FHD Gaming La...</td>\n",
       "      <td>1,99,990</td>\n",
       "      <td>2.9 out of 5 stars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HP Pavilion x360 Core i7 8th Gen 14-inch Touch...</td>\n",
       "      <td>81,990</td>\n",
       "      <td>4.1 out of 5 stars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Asus Gaming Laptop ROG Strix G17 i7-10750H(16 ...</td>\n",
       "      <td>1,20,790</td>\n",
       "      <td>4.4 out of 5 stars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(Renewed) Dell Latitude E7240 12.5-inch Laptop...</td>\n",
       "      <td>39,999</td>\n",
       "      <td>3.6 out of 5 stars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(Renewed) HP EliteBook X360 1030 G2 Laptop (Co...</td>\n",
       "      <td>49,990</td>\n",
       "      <td>2.6 out of 5 stars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(Renewed) HP EliteBook 1030 G2 X 360 Notebook ...</td>\n",
       "      <td>49,990</td>\n",
       "      <td>2.2 out of 5 stars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(Renewed) Mi Notebook Horizon Edition 14 Intel...</td>\n",
       "      <td>52,990</td>\n",
       "      <td>2.5 out of 5 stars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Lenovo Legion 5Pi 10th Gen Intel Core i7 15.6\"...</td>\n",
       "      <td>1,29,990</td>\n",
       "      <td>4.0 out of 5 stars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>HP 14s dr1006TU 14-inch Laptop (10th Gen Core ...</td>\n",
       "      <td>84,299</td>\n",
       "      <td>4.4 out of 5 stars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(Renewed) Dell Latitude E7240 12.5-inch Laptop...</td>\n",
       "      <td>39,998</td>\n",
       "      <td>2.9 out of 5 stars</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Laptop Name     Price  \\\n",
       "0  Dell Alienware m15(R3) 15.6-inch FHD Gaming La...  1,99,990   \n",
       "1  HP Pavilion x360 Core i7 8th Gen 14-inch Touch...    81,990   \n",
       "2  Asus Gaming Laptop ROG Strix G17 i7-10750H(16 ...  1,20,790   \n",
       "3  (Renewed) Dell Latitude E7240 12.5-inch Laptop...    39,999   \n",
       "4  (Renewed) HP EliteBook X360 1030 G2 Laptop (Co...    49,990   \n",
       "5  (Renewed) HP EliteBook 1030 G2 X 360 Notebook ...    49,990   \n",
       "6  (Renewed) Mi Notebook Horizon Edition 14 Intel...    52,990   \n",
       "7  Lenovo Legion 5Pi 10th Gen Intel Core i7 15.6\"...  1,29,990   \n",
       "8  HP 14s dr1006TU 14-inch Laptop (10th Gen Core ...    84,299   \n",
       "9  (Renewed) Dell Latitude E7240 12.5-inch Laptop...    39,998   \n",
       "\n",
       "               Rating  \n",
       "0  2.9 out of 5 stars  \n",
       "1  4.1 out of 5 stars  \n",
       "2  4.4 out of 5 stars  \n",
       "3  3.6 out of 5 stars  \n",
       "4  2.6 out of 5 stars  \n",
       "5  2.2 out of 5 stars  \n",
       "6  2.5 out of 5 stars  \n",
       "7  4.0 out of 5 stars  \n",
       "8  4.4 out of 5 stars  \n",
       "9  2.9 out of 5 stars  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Laptop_New =pd.concat([Title3 , Price4, Rating8] , axis=1)\n",
    "Laptop_New"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q9 :- Scrape data of first 100 sunglasses listings on flipkart.com."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries \n",
    "\n",
    "import numpy as np \n",
    "import  pandas as pd\n",
    "from selenium import webdriver\n",
    "from time import  sleep\n",
    "import time\n",
    "import string\n",
    "from random import randint\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Loading driver \n",
    "driver = webdriver.Chrome(r\"C:\\Users\\thunderbolt\\sof\\chromedriver.exe\")\n",
    "\n",
    "#Giving name of the website we have to  scrap\n",
    "url = \"https://www.flipkart.com/\"\n",
    "\n",
    "# Getting Url\n",
    "driver.get(url)\n",
    "\n",
    "time.sleep(5)\n",
    "\n",
    "Brand = []\n",
    "Product_Description = []\n",
    "Product_Description1 = []\n",
    "Price  = []\n",
    "Discount = []\n",
    "\n",
    "time.sleep(3)\n",
    "Task1 =driver.find_element_by_xpath(\"//div[@class='_3Njdz7']/button\")\n",
    "Task1.click()\n",
    "time.sleep(3)\n",
    "Task2 = driver.find_element_by_xpath(\"//div[@class='O8ZS_U']/input\")\n",
    "Task2.send_keys('Sunglasses')\n",
    "time.sleep(3)\n",
    "Task3 = driver.find_element_by_xpath(\"//button[@class='vh79eN']\")\n",
    "Task3.click()\n",
    "time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting Brand Data \n",
    "Task4 =driver.find_elements_by_xpath(\"//div[@class='_2B_pmu']\")\n",
    "for  by in Task4:\n",
    "    qd = by.text\n",
    "    Brand.append(qd)\n",
    "    \n",
    "Brand1 = pd .DataFrame(Brand)\n",
    "Brand1 = Brand1.rename(columns = { 0 : 'Brand'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(5)\n",
    "\n",
    "# Getting product description\n",
    "Task5 =driver.find_elements_by_xpath(\"//a[@class='_2mylT6']\")\n",
    "for qz in Task5:\n",
    "    mq = qz.text\n",
    "    Product_Description.append(mq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "Product_Description1 = pd.DataFrame(Product_Description)\n",
    "Product_Description1 = Product_Description1.rename(columns = { 0 : 'Product Description'}  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(5)\n",
    "#Getting Price Data\n",
    "Task6 = driver.find_elements_by_xpath(\"//div[@class='_1uv9Cb']/div[1]\")\n",
    "for qm in Task6:\n",
    "    ap = qm.text\n",
    "    Price.append(ap)\n",
    "Price1 = pd.DataFrame(Price)\n",
    "Price1 = Price1.rename(columns = { 0: 'Price'} )\n",
    "time.sleep(5)\n",
    "Task7 = driver.find_elements_by_xpath(\"//div[@class='VGWI6T']/span\") \n",
    "for cy in Task7:\n",
    "    qz =  cy.text\n",
    "    Discount.append(qz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(2)\n",
    "Discount1 = pd.DataFrame(Discount)\n",
    "Discount1 = Discount1.rename({ 0 : 'Discount'} , axis =1)\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "Brand2 = Brand1.iloc[0:38]\n",
    "Product_Description2 = Product_Description1.iloc[0:38]\n",
    "Price2 = Price1.iloc[0:38]\n",
    "Discount2 = Discount1.iloc[0:38]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(5)\n",
    "Task8 = driver.find_element_by_xpath(\"//nav[@class='_1ypTlJ']/a[2]\")\n",
    "Task8.click()\n",
    "time.sleep(5)\n",
    "\n",
    "Brand4 = []\n",
    "Product_Description4 = []\n",
    "Price4 = []\n",
    "Discount4 = []\n",
    "time.sleep(5)\n",
    "\n",
    "# Getting Brand data of second page\n",
    "Task9 =driver.find_elements_by_xpath(\"//div[@class='_2B_pmu']\")\n",
    "for  byx in Task9:\n",
    "    qdx = byx.text\n",
    "    Brand4.append(qdx)\n",
    "Brand5 = pd .DataFrame(Brand4)\n",
    "Brand6 = Brand5.rename(columns = { 0 : 'Brand'})\n",
    "time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting Production Description data\n",
    "Task10 =driver.find_elements_by_xpath(\"//a[@class='_2mylT6']\")\n",
    "Task10[0:5]\n",
    "\n",
    "for qzq in Task10:\n",
    "    mqq = qzq.text\n",
    "    Product_Description4.append(mqq)\n",
    "Product_Description5 = pd.DataFrame(Product_Description4)\n",
    "Product_Description6 = Product_Description5.rename(columns = { 0 : 'Product Description'}  )\n",
    "time.sleep(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "Task11 = driver.find_elements_by_xpath(\"//div[@class='_1uv9Cb']/div[1]\")\n",
    "for qma in Task11:\n",
    "    apa = qma.text\n",
    "    Price4.append(apa)\n",
    "Price5 = pd.DataFrame(Price4)\n",
    "Price6= Price5.rename(columns = { 0: 'Price'} )\n",
    "time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "Task12 = driver.find_elements_by_xpath(\"//div[@class='VGWI6T']/span\")\n",
    "for cyz in Task12:\n",
    "    qzz =  cyz.text\n",
    "    Discount4.append(qzz)\n",
    "Discount5 = pd.DataFrame(Discount4)\n",
    "Discount6 = Discount5.rename(columns = { 0 : 'Discount'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "Brand7 = Brand6.iloc[0:33]\n",
    "Product_Description7 = Product_Description6.iloc[0:33]\n",
    "Price7 = Price6.iloc[0:33]\n",
    "Discount7 = Discount6.iloc[0:33]\n",
    "sun1 =pd.concat([Brand7,Product_Description7,Price7,Discount7],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(4)\n",
    "Task13 = driver.find_element_by_xpath(\"//nav[@class='_1ypTlJ']/a[4]\")\n",
    "Task13.click()\n",
    "\n",
    "time.sleep(5)\n",
    "\n",
    "#Getting Brand Data from second Page\n",
    "Task14 =driver.find_elements_by_xpath(\"//div[@class='_2B_pmu']\")\n",
    "Brand10 = []\n",
    "Product_Description10 = []\n",
    "Price10 = []\n",
    "Discount10  = []\n",
    "for  byf in Task14:\n",
    "    qdf = byf.text\n",
    "    Brand10.append(qdf)\n",
    "\n",
    "Brand11 = pd .DataFrame(Brand10)\n",
    "Brand12 = Brand11.rename(columns = { 0 : 'Brand'})\n",
    "\n",
    "time.sleep(5)\n",
    "\n",
    "#getting Product description data \n",
    "Task15 =driver.find_elements_by_xpath(\"//a[@class='_2mylT6']\") \n",
    "\n",
    "for qzm in Task15:\n",
    "    mqm = qzm.text\n",
    "    Product_Description10.append(mqm)\n",
    "\n",
    "Product_Description11= pd.DataFrame(Product_Description10)\n",
    "Product_Description12 = Product_Description11.rename(columns = { 0 : 'Product Description'}  )\n",
    "time.sleep(5)\n",
    "Task16 = driver.find_elements_by_xpath(\"//div[@class='_1uv9Cb']/div[1]\")\n",
    "for qmw in Task16:\n",
    "    apw = qmw.text\n",
    "    Price10.append(apw)\n",
    "Price11 = pd.DataFrame(Price10)\n",
    "Price12 = Price11.rename(columns = { 0: 'Price'} )\n",
    "time.sleep(5)\n",
    "Task17 = driver.find_elements_by_xpath(\"//div[@class='VGWI6T']/span\") \n",
    "for cyh in Task17:\n",
    "    qzh =  cyh.text\n",
    "    Discount10.append(qzh)\n",
    "Discount11 = pd.DataFrame(Discount10)\n",
    "Discount12 = Discount11.rename({ 0 : 'Discount'} , axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Brand13 = Brand12.iloc[0:29]\n",
    "Product_Description13 = Product_Description12.iloc[0:29]\n",
    "Price13  =Price12.iloc[0:29]\n",
    "Discount13  = Discount12.iloc[0:29] \n",
    "sun2 =pd.concat([Brand13, Product_Description13,Price13, Discount13],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(4)\n",
    "Task18 = driver.find_element_by_xpath(\"//nav[@class='_1ypTlJ']/a[5]\")\n",
    "Task18.click()\n",
    "\n",
    "time.sleep(5)\n",
    "\n",
    "#Getting Brand Data from second Page\n",
    "Task19 =driver.find_elements_by_xpath(\"//div[@class='_2B_pmu']\")\n",
    "Brand15 = []\n",
    "Product_Description15 = []\n",
    "Price15 = []\n",
    "Discount15  = []\n",
    "for  byfx in Task19:\n",
    "    qdfx = byfx.text\n",
    "    Brand15.append(qdfx)\n",
    "\n",
    "Brand16 = pd .DataFrame(Brand15)\n",
    "Brand17 = Brand16.rename(columns = { 0 : 'Brand'})\n",
    "\n",
    "time.sleep(5)\n",
    "\n",
    "#getting Product description data \n",
    "Task20 =driver.find_elements_by_xpath(\"//a[@class='_2mylT6']\") \n",
    "\n",
    "for qzmq in Task20:\n",
    "    mqmq = qzmq.text\n",
    "    Product_Description15.append(mqmq)\n",
    "\n",
    "Product_Description16= pd.DataFrame(Product_Description15)\n",
    "Product_Description17 = Product_Description16.rename(columns = { 0 : 'Product Description'}  )\n",
    "time.sleep(5)\n",
    "Task21 = driver.find_elements_by_xpath(\"//div[@class='_1uv9Cb']/div[1]\")\n",
    "for qmwa in Task21:\n",
    "    apwa = qmwa.text\n",
    "    Price15.append(apwa)\n",
    "Price16 = pd.DataFrame(Price15)\n",
    "Price17 = Price16.rename(columns = { 0: 'Price'} )\n",
    "time.sleep(5)\n",
    "Task22 = driver.find_elements_by_xpath(\"//div[@class='VGWI6T']/span\") \n",
    "for cyhz in Task22:\n",
    "    qzhz =  cyhz.text\n",
    "    Discount15.append(qzhz)\n",
    "Discount16 = pd.DataFrame(Discount15)\n",
    "Discount17 = Discount16.rename({ 0 : 'Discount'} , axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "Brand18 = Brand17.iloc[0:20]\n",
    "Product_Description18 = Product_Description17.iloc[0:20]\n",
    "Price18  =Price17.iloc[0:20]\n",
    "Discount18  = Discount17.iloc[0:20] \n",
    "sun4 =pd.concat([Brand18, Product_Description18,Price18, Discount18],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brand</th>\n",
       "      <th>Product Description</th>\n",
       "      <th>Price</th>\n",
       "      <th>Discount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fossil</td>\n",
       "      <td>UV Protection Round Sunglasses (51)</td>\n",
       "      <td>₹2,640</td>\n",
       "      <td>40% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fossil</td>\n",
       "      <td>Others Rectangular Sunglasses (54)</td>\n",
       "      <td>₹2,200</td>\n",
       "      <td>50% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fastrack</td>\n",
       "      <td>UV Protection Wayfarer Sunglasses (Free Size)</td>\n",
       "      <td>₹674</td>\n",
       "      <td>25% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Elligator</td>\n",
       "      <td>UV Protection Round Sunglasses (54)</td>\n",
       "      <td>₹312</td>\n",
       "      <td>87% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Collet</td>\n",
       "      <td>Gradient, Mirrored, UV Protection Aviator Sung...</td>\n",
       "      <td>₹199</td>\n",
       "      <td>86% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>New Specs</td>\n",
       "      <td>Gradient Rectangular Sunglasses (53)</td>\n",
       "      <td>₹296</td>\n",
       "      <td>78% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>shah collections</td>\n",
       "      <td>Mirrored Aviator Sunglasses (55)</td>\n",
       "      <td>₹214</td>\n",
       "      <td>78% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>PIRASO</td>\n",
       "      <td>Gradient, UV Protection, Mirrored Round Sungla...</td>\n",
       "      <td>₹211</td>\n",
       "      <td>86% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Fastrack</td>\n",
       "      <td>Mirrored Round Sunglasses (53)</td>\n",
       "      <td>₹1,039</td>\n",
       "      <td>20% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Fossil</td>\n",
       "      <td>UV Protection Wayfarer Sunglasses (Free Size)</td>\n",
       "      <td>₹1,980</td>\n",
       "      <td>55% off</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               Brand                                Product Description  \\\n",
       "0             Fossil                UV Protection Round Sunglasses (51)   \n",
       "1             Fossil                 Others Rectangular Sunglasses (54)   \n",
       "2           Fastrack      UV Protection Wayfarer Sunglasses (Free Size)   \n",
       "3          Elligator                UV Protection Round Sunglasses (54)   \n",
       "4             Collet  Gradient, Mirrored, UV Protection Aviator Sung...   \n",
       "..               ...                                                ...   \n",
       "15         New Specs               Gradient Rectangular Sunglasses (53)   \n",
       "16  shah collections                   Mirrored Aviator Sunglasses (55)   \n",
       "17            PIRASO  Gradient, UV Protection, Mirrored Round Sungla...   \n",
       "18          Fastrack                     Mirrored Round Sunglasses (53)   \n",
       "19            Fossil      UV Protection Wayfarer Sunglasses (Free Size)   \n",
       "\n",
       "     Price Discount  \n",
       "0   ₹2,640  40% off  \n",
       "1   ₹2,200  50% off  \n",
       "2     ₹674  25% off  \n",
       "3     ₹312  87% off  \n",
       "4     ₹199  86% off  \n",
       "..     ...      ...  \n",
       "15    ₹296  78% off  \n",
       "16    ₹214  78% off  \n",
       "17    ₹211  86% off  \n",
       "18  ₹1,039  20% off  \n",
       "19  ₹1,980  55% off  \n",
       "\n",
       "[120 rows x 4 columns]"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting Final Sunglasses data \n",
    "Sunglasses = pd.concat([Sun,sun1,sun2,sun4] , axis=0)\n",
    "Sunglasses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10 - Scrape 100 reviews data from flipkart.com for iphone11 phone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import os\n",
    "import pandas as pd\n",
    "from time import  sleep\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"C:\\Users\\thunderbolt\\sof\\chromedriver.exe\"\n",
    "\n",
    "driver=webdriver.Chrome(path)\n",
    "my_page=driver.get('https://www.flipkart.com/apple-iphone-11-black-64-gb-includes-earpods-power-adapter/p/itm0f37c2240b217?pid=MOBFKCTSVZAXUHGR&lid=LSTMOBFKCTSVZAXUHGREPBFGI&marketplace')\n",
    "\n",
    "time.sleep(3)\n",
    "\n",
    "insert=driver.find_element_by_xpath(\"//div[@class='swINJg _3nrCtb']/span\").click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 10 10\n"
     ]
    }
   ],
   "source": [
    "rating=driver.find_elements_by_xpath(\"//div[@class='hGSR34 E_uFuv']\") \n",
    "\n",
    "review_summary=driver.find_elements_by_xpath(\"//p[@class='_2xg6Ul']\")\n",
    "\n",
    "full_review=driver.find_elements_by_xpath(\"//div[@class='qwjRop']\")\n",
    "\n",
    "print(len(rating),len(review_summary),len(full_review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Rating': [], 'Review_summary': [], 'Full_review': []}"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "I_phone={}\n",
    "I_phone['Rating']=[]\n",
    "I_phone['Review_summary']=[]\n",
    "I_phone['Full_review']=[]\n",
    "I_phone\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "10\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "for ab in rating:\n",
    "    I_phone['Rating'].append(ab.text)\n",
    "print(len(I_phone['Rating']))\n",
    "\n",
    "for bc in review_summary:\n",
    "    I_phone['Review_summary'].append(bc.text)\n",
    "print(len(I_phone['Review_summary']))\n",
    "\n",
    "for cd in full_review:\n",
    "    I_phone['Full_review'].append(cd.text)\n",
    "print(len(I_phone['Full_review']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_page=driver.find_element_by_xpath(\"//nav[@class='_1ypTlJ']/a[4]\").click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 10 10\n"
     ]
    }
   ],
   "source": [
    "rating=driver.find_elements_by_xpath(\"//div[@class='hGSR34 E_uFuv']\") \n",
    "\n",
    "review_summary=driver.find_elements_by_xpath(\"//p[@class='_2xg6Ul']\")\n",
    "\n",
    "full_review=driver.find_elements_by_xpath(\"//div[@class='qwjRop']\")\n",
    "\n",
    "print(len(rating),len(review_summary),len(full_review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "20\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "for cc in rating:\n",
    "    I_phone['Rating'].append(cc.text)\n",
    "print(len(I_phone['Rating']))\n",
    "\n",
    "for bcc in review_summary:\n",
    "    I_phone['Review_summary'].append(bcc.text)\n",
    "print(len(I_phone['Review_summary']))\n",
    "\n",
    "for cdc in full_review:\n",
    "    I_phone['Full_review'].append(cdc.text)\n",
    "print(len(I_phone['Full_review']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_page=driver.find_element_by_xpath(\"//nav[@class='_1ypTlJ']/a[5]\").click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 10 10\n"
     ]
    }
   ],
   "source": [
    "rating=driver.find_elements_by_xpath(\"//div[@class='hGSR34 E_uFuv']\") \n",
    "\n",
    "review_summary=driver.find_elements_by_xpath(\"//p[@class='_2xg6Ul']\")\n",
    "\n",
    "full_review=driver.find_elements_by_xpath(\"//div[@class='qwjRop']\")\n",
    "\n",
    "print(len(rating),len(review_summary),len(full_review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "30\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "for acc in rating:\n",
    "    I_phone['Rating'].append(acc.text)\n",
    "print(len(I_phone['Rating']))\n",
    "\n",
    "for zcc in review_summary:\n",
    "    I_phone['Review_summary'].append(zcc.text)\n",
    "print(len(I_phone['Review_summary']))\n",
    "\n",
    "for odc in full_review:\n",
    "    I_phone['Full_review'].append(odc.text)\n",
    "print(len(I_phone['Full_review']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_page=driver.find_element_by_xpath(\"//nav[@class='_1ypTlJ']/a[6]\").click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 10 10\n"
     ]
    }
   ],
   "source": [
    "rating=driver.find_elements_by_xpath(\"//div[@class='hGSR34 E_uFuv']\") \n",
    "\n",
    "review_summary=driver.find_elements_by_xpath(\"//p[@class='_2xg6Ul']\")\n",
    "\n",
    "full_review=driver.find_elements_by_xpath(\"//div[@class='qwjRop']\")\n",
    "\n",
    "print(len(rating),len(review_summary),len(full_review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n",
      "40\n",
      "40\n"
     ]
    }
   ],
   "source": [
    "for acc in rating:\n",
    "    I_phone['Rating'].append(acc.text)\n",
    "print(len(I_phone['Rating']))\n",
    "\n",
    "for zcc in review_summary:\n",
    "    I_phone['Review_summary'].append(zcc.text)\n",
    "print(len(I_phone['Review_summary']))\n",
    "\n",
    "for odc in full_review:\n",
    "    I_phone['Full_review'].append(odc.text)\n",
    "print(len(I_phone['Full_review']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_page=driver.find_element_by_xpath(\"//nav[@class='_1ypTlJ']/a[7]\").click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 10 10\n"
     ]
    }
   ],
   "source": [
    "rating=driver.find_elements_by_xpath(\"//div[@class='hGSR34 E_uFuv']\") \n",
    "\n",
    "review_summary=driver.find_elements_by_xpath(\"//p[@class='_2xg6Ul']\")\n",
    "\n",
    "full_review=driver.find_elements_by_xpath(\"//div[@class='qwjRop']\")\n",
    "\n",
    "print(len(rating),len(review_summary),len(full_review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "50\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "for ab in rating:\n",
    "    I_phone['Rating'].append(ab.text)\n",
    "print(len(I_phone['Rating']))\n",
    "\n",
    "for bc in review_summary:\n",
    "    I_phone['Review_summary'].append(bc.text)\n",
    "print(len(I_phone['Review_summary']))\n",
    "\n",
    "for cd in full_review:\n",
    "    I_phone['Full_review'].append(cd.text)\n",
    "print(len(I_phone['Full_review']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_page=driver.find_element_by_xpath(\"//nav[@class='_1ypTlJ']/a[8]\").click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 10 10\n"
     ]
    }
   ],
   "source": [
    "rating=driver.find_elements_by_xpath(\"//div[@class='hGSR34 E_uFuv']\") \n",
    "\n",
    "review_summary=driver.find_elements_by_xpath(\"//p[@class='_2xg6Ul']\")\n",
    "\n",
    "full_review=driver.find_elements_by_xpath(\"//div[@class='qwjRop']\")\n",
    "\n",
    "print(len(rating),len(review_summary),len(full_review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n",
      "60\n",
      "60\n"
     ]
    }
   ],
   "source": [
    "for ab in rating:\n",
    "    I_phone['Rating'].append(ab.text)\n",
    "print(len(I_phone['Rating']))\n",
    "\n",
    "for bc in review_summary:\n",
    "    I_phone['Review_summary'].append(bc.text)\n",
    "print(len(I_phone['Review_summary']))\n",
    "\n",
    "for cd in full_review:\n",
    "    I_phone['Full_review'].append(cd.text)\n",
    "print(len(I_phone['Full_review']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_page=driver.find_element_by_xpath(\"//nav[@class='_1ypTlJ']/a[9]\").click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 10 10\n"
     ]
    }
   ],
   "source": [
    "rating=driver.find_elements_by_xpath(\"//div[@class='hGSR34 E_uFuv']\") \n",
    "\n",
    "review_summary=driver.find_elements_by_xpath(\"//p[@class='_2xg6Ul']\")\n",
    "\n",
    "full_review=driver.find_elements_by_xpath(\"//div[@class='qwjRop']\")\n",
    "\n",
    "print(len(rating),len(review_summary),len(full_review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70\n",
      "70\n",
      "70\n"
     ]
    }
   ],
   "source": [
    "for ab in rating:\n",
    "    I_phone['Rating'].append(ab.text)\n",
    "print(len(I_phone['Rating']))\n",
    "\n",
    "for bc in review_summary:\n",
    "    I_phone['Review_summary'].append(bc.text)\n",
    "print(len(I_phone['Review_summary']))\n",
    "\n",
    "for cd in full_review:\n",
    "    I_phone['Full_review'].append(cd.text)\n",
    "print(len(I_phone['Full_review']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_page=driver.find_element_by_xpath(\"//nav[@class='_1ypTlJ']/a[10]\").click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 10 10\n"
     ]
    }
   ],
   "source": [
    "rating=driver.find_elements_by_xpath(\"//div[@class='hGSR34 E_uFuv']\") \n",
    "\n",
    "review_summary=driver.find_elements_by_xpath(\"//p[@class='_2xg6Ul']\")\n",
    "\n",
    "full_review=driver.find_elements_by_xpath(\"//div[@class='qwjRop']\")\n",
    "\n",
    "print(len(rating),len(review_summary),len(full_review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80\n",
      "80\n",
      "80\n"
     ]
    }
   ],
   "source": [
    "for ab in rating:\n",
    "    I_phone['Rating'].append(ab.text)\n",
    "print(len(I_phone['Rating']))\n",
    "\n",
    "for bc in review_summary:\n",
    "    I_phone['Review_summary'].append(bc.text)\n",
    "print(len(I_phone['Review_summary']))\n",
    "\n",
    "for cd in full_review:\n",
    "    I_phone['Full_review'].append(cd.text)\n",
    "print(len(I_phone['Full_review']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_page=driver.find_element_by_xpath(\"//nav[@class='_1ypTlJ']/a[12]\").click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 10 10\n"
     ]
    }
   ],
   "source": [
    "rating=driver.find_elements_by_xpath(\"//div[@class='hGSR34 E_uFuv']\") \n",
    "\n",
    "review_summary=driver.find_elements_by_xpath(\"//p[@class='_2xg6Ul']\")\n",
    "\n",
    "full_review=driver.find_elements_by_xpath(\"//div[@class='qwjRop']\")\n",
    "\n",
    "print(len(rating),len(review_summary),len(full_review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90\n",
      "90\n",
      "90\n"
     ]
    }
   ],
   "source": [
    "for ab in rating:\n",
    "    I_phone['Rating'].append(ab.text)\n",
    "print(len(I_phone['Rating']))\n",
    "\n",
    "for bc in review_summary:\n",
    "    I_phone['Review_summary'].append(bc.text)\n",
    "print(len(I_phone['Review_summary']))\n",
    "\n",
    "for cd in full_review:\n",
    "    I_phone['Full_review'].append(cd.text)\n",
    "print(len(I_phone['Full_review']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_page=driver.find_element_by_xpath(\"//nav[@class='_1ypTlJ']/a[11]\").click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 10 10\n"
     ]
    }
   ],
   "source": [
    "rating=driver.find_elements_by_xpath(\"//div[@class='hGSR34 E_uFuv']\") \n",
    "\n",
    "review_summary=driver.find_elements_by_xpath(\"//p[@class='_2xg6Ul']\")\n",
    "\n",
    "full_review=driver.find_elements_by_xpath(\"//div[@class='qwjRop']\")\n",
    "\n",
    "print(len(rating),len(review_summary),len(full_review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "100\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "for ab in rating:\n",
    "    I_phone['Rating'].append(ab.text)\n",
    "print(len(I_phone['Rating']))\n",
    "\n",
    "for bc in review_summary:\n",
    "    I_phone['Review_summary'].append(bc.text)\n",
    "print(len(I_phone['Review_summary']))\n",
    "\n",
    "for cd in full_review:\n",
    "    I_phone['Full_review'].append(cd.text)\n",
    "print(len(I_phone['Full_review']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rating</th>\n",
       "      <th>Review_summary</th>\n",
       "      <th>Full_review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>Perfect product!</td>\n",
       "      <td>Amazing phone with great cameras and better ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>Great product</td>\n",
       "      <td>Amazing Powerful and Durable Gadget.\\n\\nI’m am...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>Perfect product!</td>\n",
       "      <td>It’s a must buy who is looking for an upgrade ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>Highly recommended</td>\n",
       "      <td>iphone 11 is a very good phone to buy only if ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Perfect product!</td>\n",
       "      <td>Value for money❤️❤️\\nIts awesome mobile phone ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>5</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>100% Original</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>5</td>\n",
       "      <td>Terrific purchase</td>\n",
       "      <td>No word to say ..simply Awesome. Love it😘😘😘😘</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>5</td>\n",
       "      <td>Worth every penny</td>\n",
       "      <td>Excellent phone it’s a pro and value for the m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>5</td>\n",
       "      <td>Best in the market!</td>\n",
       "      <td>Best Camera, good speakers and I guess green i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>5</td>\n",
       "      <td>Worth every penny</td>\n",
       "      <td>Really a giant for battery backup and really g...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rating       Review_summary  \\\n",
       "0       5     Perfect product!   \n",
       "1       5        Great product   \n",
       "2       5     Perfect product!   \n",
       "3       5   Highly recommended   \n",
       "4       5     Perfect product!   \n",
       "..    ...                  ...   \n",
       "95      5            Excellent   \n",
       "96      5    Terrific purchase   \n",
       "97      5    Worth every penny   \n",
       "98      5  Best in the market!   \n",
       "99      5    Worth every penny   \n",
       "\n",
       "                                          Full_review  \n",
       "0   Amazing phone with great cameras and better ba...  \n",
       "1   Amazing Powerful and Durable Gadget.\\n\\nI’m am...  \n",
       "2   It’s a must buy who is looking for an upgrade ...  \n",
       "3   iphone 11 is a very good phone to buy only if ...  \n",
       "4   Value for money❤️❤️\\nIts awesome mobile phone ...  \n",
       "..                                                ...  \n",
       "95                                      100% Original  \n",
       "96       No word to say ..simply Awesome. Love it😘😘😘😘  \n",
       "97  Excellent phone it’s a pro and value for the m...  \n",
       "98  Best Camera, good speakers and I guess green i...  \n",
       "99  Really a giant for battery backup and really g...  \n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.DataFrame(I_phone)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
